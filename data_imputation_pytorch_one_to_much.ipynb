{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install plotly\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las siguientes funciones para el preprocesamiento de los datos\n",
    "#Con la siguiente funciona nos aseguramos que los datos de entrada tengan el formato correcto para el funcionamiento de las redes recurrentes \n",
    "def create_sequences(data, lookback=24, forecast_horizon=6):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        x = data[i:(i + lookback)]\n",
    "        y = data[(i + lookback):(i + lookback + forecast_horizon)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return torch.tensor(xs, dtype=torch.float32), torch.tensor(ys, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#Con la siguiente funcion analizamos los datos duplicados en el indice del dataframe \n",
    "def analyze_duplicate_indices(df):\n",
    "    # Identificar índices duplicados\n",
    "    duplicated_indices = df.index[df.index.duplicated(keep=False)]\n",
    "    \n",
    "    if duplicated_indices.empty:\n",
    "        print(\"No se encontraron índices duplicados.\")\n",
    "        return None\n",
    "\n",
    "    # Ordenar los duplicados\n",
    "    duplicates_sorted = df.loc[duplicated_indices].sort_index()\n",
    "\n",
    "    # Agrupar los duplicados\n",
    "    grouped_duplicates = duplicates_sorted.groupby(duplicates_sorted.index)\n",
    "\n",
    "    duplicate_info = []\n",
    "\n",
    "    for idx, group in grouped_duplicates:\n",
    "        duplicate_info.append({\n",
    "            'index': idx,\n",
    "            'count': len(group),\n",
    "            'values': group.values.flatten().tolist()\n",
    "        })\n",
    "\n",
    "    return duplicate_info\n",
    "\n",
    "def remove_duplicate_indices(df, method='first'):\n",
    "    # Analizar duplicados\n",
    "    duplicate_info = analyze_duplicate_indices(df)\n",
    "    \n",
    "    if not duplicate_info:\n",
    "        print(\"No hay índices duplicados para eliminar.\")\n",
    "        return df\n",
    "    \n",
    "    # Eliminar duplicados\n",
    "    df_sin_duplicados = df[~df.index.duplicated(keep=method)]\n",
    "    \n",
    "    print(f\"Se eliminaron {len(df) - len(df_sin_duplicados)} filas con índices duplicados.\")\n",
    "    print(f\"Tamaño original del DataFrame: {len(df)}\")\n",
    "    print(f\"Tamaño del DataFrame sin duplicados: {len(df_sin_duplicados)}\")\n",
    "    \n",
    "    return df_sin_duplicados\n",
    "\n",
    "def find_nan_sequences(df, max_consecutive_nans=120, view=False):\n",
    "    nan_index = df[df.isnull().any(axis=1)].index\n",
    "    # print(nan_index)\n",
    "    nan_sequences = []\n",
    "    \n",
    "    if len(nan_index) == 0:\n",
    "        return nan_sequences\n",
    "\n",
    "    start_date = nan_index[0]\n",
    "    prev_date = nan_index[0]\n",
    "    \n",
    "    for i in range(1, len(nan_index)):\n",
    "        current_date = nan_index[i]\n",
    "        \n",
    "        if current_date - prev_date != pd.Timedelta('1 hour 00:00:00'):\n",
    "            # No es consecutivo, guardar la secuencia anterior\n",
    "            end_date = prev_date\n",
    "            num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "            \n",
    "            if num_nans == 1:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'single',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            elif num_nans <= max_consecutive_nans:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'consecutive',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            elif num_nans > max_consecutive_nans:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'long_consecutive',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            else:\n",
    "                if view:\n",
    "                    print(f'Secuencia de NaN mayor a {max_consecutive_nans} horas: {start_date} a {end_date}, {num_nans} valores')\n",
    "            \n",
    "            # Iniciar una nueva secuencia\n",
    "            start_date = current_date\n",
    "        \n",
    "        prev_date = current_date\n",
    "    \n",
    "    # Manejar la última secuencia\n",
    "    end_date = prev_date\n",
    "    num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "    \n",
    "    if num_nans == 1:\n",
    "        nan_sequences.append({\n",
    "            'type': 'single',\n",
    "            'date': start_date\n",
    "        })\n",
    "    elif num_nans <= max_consecutive_nans:\n",
    "        nan_sequences.append({\n",
    "            'type': 'consecutive',\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'num_nans': num_nans\n",
    "        })\n",
    "    else:\n",
    "        if view:\n",
    "            print(f'Secuencia de NaN mayor a {max_consecutive_nans} horas: {start_date} a {end_date}, {num_nans} valores')\n",
    "    \n",
    "    return nan_sequences\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la variable imputation_lluvia_aitsu en otra variable para las modificaciones\n",
    "variable = 'CAUDAL_RIO_Aitzu-Urola'\n",
    "data_river_2 = pd.read_csv('db_21.csv')\n",
    "data_river_2 = data_river_2.set_index('Fecha')\n",
    "# Asignar un formato de fecha a la columna fecha\n",
    "data_river_2.index = pd.to_datetime(data_river_2.index, format='%Y-%m-%d %H:%M:%S')\n",
    "# Se trabajara sobre la variable de nivel del rio Aitzu-Urola \n",
    "imputation_aitsu = data_river_2[[variable]].copy()\n",
    "date_init = '1999-03-16 17:00'\n",
    "date_end = '2023-09-30 23:00'\n",
    "imputation_aitsu = imputation_aitsu[date_init:date_end] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imputation_aitsu.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aitzu_image = data_river_2.copy()\n",
    "plt.figure(figsize=(100, 40))\n",
    "plt.plot(list(aitzu_image['CAUDAL_RIO_Aitzu-Urola']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función\n",
    "# duplicate_info = analyze_duplicate_indices(imputation_lluvia_aitsu)\n",
    "\n",
    "# Usar la función\n",
    "# imputation_lluvia_aitsu_sin_duplicados = remove_duplicate_indices(imputation_lluvia_aitsu, method='first')\n",
    "\n",
    "# Uso de la función\n",
    "nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=True)\n",
    "\n",
    "# Imprimir resultados\n",
    "for x in nan_sequences:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar la secuencia de datos para el entrenamiento\n",
    "x = 0\n",
    "# init_train = nan_sequences[0]['end_date'] + pd.Timedelta('1 hour 00:00:00')\n",
    "end_train = nan_sequences[x]['start_date'] - pd.Timedelta('1 hour 00:00:00')\n",
    "data_river = imputation_aitsu[:end_train]\n",
    "print('Fecha de inicio para el entrenamiento ----> ', data_river.index[0])\n",
    "print('Fecha de fin del entrenamiento ------> ', data_river.index[-1])\n",
    "\n",
    "total_size = len(data_river)\n",
    "train_size = int(total_size * 0.70)\n",
    "val_size = int(total_size * 0.15)\n",
    "test_size = total_size - train_size - val_size\n",
    "train = data_river[:train_size]\n",
    "val = data_river[train_size:train_size + val_size]\n",
    "test = data_river[train_size + val_size:]\n",
    "\n",
    "print('Numero de valores para el train:{}, val:{} y test:{}'.format(train.shape, val.shape, test.shape))\n",
    "print(data_river.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion de los datos\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar los conjuntos\n",
    "train_scaled = scaler.transform(train.values)\n",
    "val_scaled = scaler.transform(val.values)\n",
    "test_scaled = scaler.transform(test.values)\n",
    "\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=[variable], index=train.index)\n",
    "val_scaled = pd.DataFrame(val_scaled, columns=[variable], index=val.index)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=[variable], index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las secuencias de entrenamiento, validación y test\n",
    "lookback = 24\n",
    "forecast_horizon = 6\n",
    "x_train, y_train = create_sequences(train_scaled.values, lookback, forecast_horizon)\n",
    "x_val, y_val = create_sequences(val_scaled.values, lookback, forecast_horizon)\n",
    "x_test, y_test = create_sequences(test_scaled.values, lookback, forecast_horizon)\n",
    "print('Tamaño de las secuencias:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
    "#         self.linear = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "\n",
    "# class ImprovedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size = 1, hidden_size = 64, num_layers = 2, dropout = 0.2):\n",
    "#         super(ImprovedLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out.unsqueeze(1)\n",
    "    \n",
    "class ImprovedLSTM(nn.Module):\n",
    "       def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "           super(ImprovedLSTM, self).__init__()\n",
    "           self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "           self.fc = nn.Linear(hidden_size, 32)\n",
    "           self.relu = nn.ReLU()\n",
    "           self.dropout = nn.Dropout(dropout)\n",
    "           self.output = nn.Linear(32, output_horizon)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           out, _ = self.lstm(x)\n",
    "           out = self.fc(out[:, -1, :])\n",
    "           out = self.relu(out)\n",
    "           out = self.dropout(out)\n",
    "           out = self.output(out)\n",
    "           return out\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_horizon = forecast_horizon\n",
    "dropout = 0.2\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# class ImprovedModel(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "#                             num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.linear(out[:, -1, :])  # Toma solo el último paso temporal\n",
    "#         return out.unsqueeze(1)\n",
    "\n",
    "# model = ImprovedLSTM(input_size=1, hidden_size=64, num_layers=2, dropout=0.2)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "weight_decay = 1e-5\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "scheduler_patience = 10\n",
    "early_stopping_patience = 20\n",
    "n_epochs = 1000\n",
    "validation_freq = 1\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "output_horizon = forecast_horizon\n",
    "dropout = 0.3\n",
    "\n",
    "x_train = x_train.to(device).float()    \n",
    "y_train = y_train.to(device).float()\n",
    "x_val = x_val.to(device).float()\n",
    "y_val = y_val.to(device).float()\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=scheduler_patience, threshold=0.00001)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "val_rmse = []\n",
    "val_maes = []\n",
    "val_mapes = []\n",
    "learning_rates = []\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_epoch = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pred_train = []\n",
    "pred_val = []\n",
    "pred_test = []\n",
    "\n",
    "for epoch in range (n_epochs):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)  # Add an underscore to ignore the hidden state\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append(loss.item())\n",
    "    \n",
    "    epoch_train_loss = np.mean(epoch_train_losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    #Validacion\n",
    "    if epoch % validation_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(x_val)\n",
    "            val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # Corregir la forma de los datos antes de aplicar inverse_transform\n",
    "            y_true = scaler.inverse_transform(y_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "            y_pred_np = scaler.inverse_transform(y_pred_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "            rmse, mae, mape = calculate_metrics(y_true.flatten(), y_pred_np.flatten())\n",
    "            val_rmse.append(rmse)\n",
    "            val_maes.append(mae)\n",
    "            val_mapes.append(mape)\n",
    "\n",
    "    scheduler.step(val_loss.item())\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {epoch_train_loss:.6f}, Val Loss: {val_loss.item():.6f}, Val RMSE: {rmse:.6f}, Val MAE: {mae:.6f}, Val MAPE: {mape:.6f}, LR: {current_lr:.8f}, # Epoch total: {n_epochs}')\n",
    "        \n",
    "        # Early stopping\n",
    "    if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            counter = 0\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            torch.save(best_model, 'best_model_aitsu.pth')\n",
    "    else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nEntrenamiento completado en {total_time:.2f} segundos')\n",
    "print(f'Tiempo promedio por época: {total_time/n_epochs:.2f} segundos')\n",
    "print(f\"Best model was saved at epoch {best_epoch} with validation RMSE: {best_val_loss:.4f}\")\n",
    "\n",
    "# Evaluacion en el conjunto de test\n",
    "# Evaluación en el conjunto de test\n",
    "model.load_state_dict(torch.load('best_model_aitsu.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(x_test.to(device).float())\n",
    "    y_true_test = scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "    y_pred_test_np = scaler.inverse_transform(y_pred_test.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "    rmse_test, mae_test, mape_test = calculate_metrics(y_true_test.flatten(), y_pred_test_np.flatten())\n",
    "    print(f'Test RMSE: {rmse_test:.6f}, Test MAE: {mae_test:.6f}, Test MAPE: {mape_test:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(0, n_epochs, 1))  # Asumiendo que guardamos métricas cada 100 épocas\n",
    "train_losses_plot = train_losses  # Tomamos cada 100º elemento\n",
    "val_losses_plot = val_losses\n",
    "val_rmses_plot = val_rmse\n",
    "learning_rates_plot = learning_rates\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                    subplot_titles=('Loss', 'Metrics', 'Learning Rate', 'predictions'),\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "# Añadir pérdidas de entrenamiento y validación\n",
    "fig.add_trace(go.Scatter(x=epochs, y=train_losses_plot, mode='lines', name='Training Loss',\n",
    "                         line=dict(color='blue', width=2)),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_losses_plot, mode='lines', name='Validation Loss',\n",
    "                         line=dict(color='red', width=2)),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Añadir RMSE, MAE y MAPE\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_rmses_plot, mode='lines', name='RMSE',\n",
    "                         line=dict(color='green', width=2)),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_maes, mode='lines', name='MAE',\n",
    "                         line=dict(color='orange', width=2)),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_mapes, mode='lines', name='MAPE',\n",
    "                         line=dict(color='purple', width=2)),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Añadir tasa de aprendizaje\n",
    "fig.add_trace(go.Scatter(x=epochs, y=learning_rates_plot, mode='lines', name='Learning Rate',\n",
    "                         line=dict(color='brown', width=2)),\n",
    "              row=3, col=1)\n",
    "\n",
    "# # Predicciones de train y val\n",
    "# fig.add_trace(go.Scatter(x=train.index, y=np.array(pred_train).flatten(), mode='lines', name='Train Predictions',\n",
    "#                          line=dict(color='blue', width=2)),\n",
    "#               row=4, col=1)\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=val.index, y=np.array(pred_val).flatten(), mode='lines', name='Val Predictions',\n",
    "#                          line=dict(color='red', width=2)),\n",
    "#               row=4, col=1)\n",
    "\n",
    "\n",
    "# Actualizar el diseño\n",
    "fig.update_layout(height=1200, width=1400, title_text=\"Model Training Metrics\")\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Metric Value\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Learning Rate\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Predictions\", row=4, col=1)\n",
    "\n",
    "# Mostrar la figura\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los arrays para el plot\n",
    "data_river_scaled = pd.concat([train_scaled, val_scaled, test_scaled])\n",
    "data_river_scaled_values = data_river_scaled.values  # Para mayor claridad\n",
    "total_length = len(data_river_scaled_values)\n",
    "train_plot = np.ones((total_length, 1)) * np.nan\n",
    "val_plot = np.ones((total_length, 1)) * np.nan\n",
    "test_plot = np.ones((total_length, 1)) * np.nan\n",
    "\n",
    "# Predicciones para el conjunto de entrenamiento\n",
    "y_pred_train = model(x_train).detach().cpu().numpy()\n",
    "y_pred_train = y_pred_train.reshape(-1, forecast_horizon)\n",
    "start_idx = lookback + forecast_horizon - 1\n",
    "end_idx = start_idx + len(y_pred_train)\n",
    "train_plot[start_idx:end_idx] = y_pred_train[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Predicciones para el conjunto de validación\n",
    "y_pred_val = model(x_val.to(device)).detach().cpu().numpy()\n",
    "y_pred_val = y_pred_val.reshape(-1, forecast_horizon)\n",
    "val_start_idx = train_size + lookback + forecast_horizon - 1\n",
    "val_end_idx = val_start_idx + len(y_pred_val)\n",
    "val_plot[val_start_idx:val_end_idx] = y_pred_val[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Predicciones para el conjunto de test\n",
    "y_pred_test = model(x_test.to(device)).detach().cpu().numpy()\n",
    "y_pred_test = y_pred_test.reshape(-1, forecast_horizon)\n",
    "test_start_idx = train_size + val_size + lookback + forecast_horizon - 1\n",
    "test_end_idx = test_start_idx + len(y_pred_test)\n",
    "test_plot[test_start_idx:test_end_idx] = y_pred_test[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Invertir la normalización\n",
    "train_plot = scaler.inverse_transform(train_plot)\n",
    "val_plot = scaler.inverse_transform(val_plot)\n",
    "test_plot = scaler.inverse_transform(test_plot)\n",
    "true_values = scaler.inverse_transform(data_river_scaled_values)\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(data_river_scaled.index, true_values, label='True', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, train_plot, label='Train Predictions', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, val_plot, label='Validation Predictions', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, test_plot, label='Test Predictions', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Model Predictions vs True Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('River Level')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade nbformat\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Preparar los arrays para el plot\n",
    "    data_river_scaled = pd.concat([train_scaled, val_scaled, test_scaled])\n",
    "    total_length = len(data_river_scaled)\n",
    "    train_plot = np.ones((total_length, 1)) * np.nan\n",
    "    val_plot = np.ones((total_length, 1)) * np.nan\n",
    "    test_plot = np.ones((total_length, 1)) * np.nan\n",
    "\n",
    "    # Predicciones para el conjunto de entrenamiento\n",
    "    x_train_gpu = x_train.to(device)\n",
    "    y_pred_train = model(x_train_gpu).cpu().numpy()\n",
    "    y_pred_train_last = y_pred_train[:, -1]\n",
    "    train_start_idx = lookback\n",
    "    train_end_idx = train_start_idx + len(y_pred_train_last)\n",
    "    train_plot[train_start_idx:train_end_idx] = y_pred_train_last.reshape(-1, 1)\n",
    "\n",
    "    # Predicciones para el conjunto de validación\n",
    "    x_val_gpu = x_val.to(device)\n",
    "    y_pred_val = model(x_val_gpu).cpu().numpy()\n",
    "    y_pred_val_last = y_pred_val[:, -1]\n",
    "    val_start_idx = train_size + lookback\n",
    "    val_end_idx = val_start_idx + len(y_pred_val_last)\n",
    "    val_plot[val_start_idx:val_end_idx] = y_pred_val_last.reshape(-1, 1)\n",
    "\n",
    "    # Predicciones para el conjunto de test\n",
    "    x_test_gpu = x_test.to(device)\n",
    "    y_pred_test = model(x_test_gpu).cpu().numpy()\n",
    "    y_pred_test_last = y_pred_test[:, -1]\n",
    "    test_start_idx = train_size + val_size + lookback\n",
    "    test_end_idx = test_start_idx + len(y_pred_test_last)\n",
    "    test_plot[test_start_idx:test_end_idx] = y_pred_test_last.reshape(-1, 1)\n",
    "\n",
    "    # Invertir la normalización\n",
    "    train_plot = scaler.inverse_transform(train_plot)\n",
    "    val_plot = scaler.inverse_transform(val_plot)\n",
    "    test_plot = scaler.inverse_transform(test_plot)\n",
    "    true_values = scaler.inverse_transform(data_river_scaled.values)\n",
    "\n",
    "\n",
    "# Crear el gráfico con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Añadir los datos reales\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=true_values.flatten(),\n",
    "                         mode='lines', name='Datos Reales', line=dict(color='blue', width=2)))\n",
    "\n",
    "# Añadir las predicciones de entrenamiento\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=train_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Train', line=dict(color='red', width=2)))\n",
    "\n",
    "# Añadir las predicciones de validación\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=val_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Val', line=dict(color='green', width=2)))\n",
    "\n",
    "# Añadir las predicciones de test\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=test_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Test', line=dict(color='purple', width=2)))\n",
    "\n",
    "# Configurar el diseño del gráfico\n",
    "fig.update_layout(\n",
    "    title='Predicciones del Modelo vs Valores Reales',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Nivel del Río',\n",
    "    legend_title='Leyenda',\n",
    "    width=2600,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para entrenamiento\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    train_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = train_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_train) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    train_plot_step[start_idx:start_idx + max_length] = y_pred_train[:max_length, step].reshape(-1, 1)\n",
    "    train_plot_step = scaler.inverse_transform(train_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=train_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Train Step {step + 1}',\n",
    "        line=dict(width=1),\n",
    "        opacity=0.8\n",
    "    ))\n",
    "\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para validación\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    val_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = val_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_val) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    val_plot_step[start_idx:start_idx + max_length] = y_pred_val[:max_length, step].reshape(-1, 1)\n",
    "    val_plot_step = scaler.inverse_transform(val_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=val_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Val Step {step + 1}',\n",
    "        line=dict(width=1, dash='dash'),\n",
    "        opacity=0.85\n",
    "    ))\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para test\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    test_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = test_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_test) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    test_plot_step[start_idx:start_idx + max_length] = y_pred_test[:max_length, step].reshape(-1, 1)\n",
    "    test_plot_step = scaler.inverse_transform(test_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=test_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Test Step {step + 1}',\n",
    "        line=dict(width=1, dash='dot'),\n",
    "        opacity=0.9\n",
    "    ))\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, last_known_sequence, data_miss):\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    current_sequence = last_known_sequence.clone().to(device)\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            prediction = model(current_sequence.float())\n",
    "            future_predictions.append(prediction[0, -1].item())\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    return future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future_autoregressive(model, last_known_sequence, data_miss, scaler, original_data, device):\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    # Convertir la secuencia conocida en un tensor de PyTorch\n",
    "    current_sequence = torch.tensor(last_known_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            # Generar la predicción\n",
    "            prediction = model(current_sequence.float())\n",
    "            prediction = prediction.unsqueeze(-1)  # Aseguramos que prediction sea 3D\n",
    "\n",
    "            # Extraer la primera predicción y añadirla a las predicciones futuras\n",
    "            future_predictions.append(prediction[0, -1, 0].item())\n",
    "\n",
    "            # Actualizar la secuencia actual con la predicción generada\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    # Invertir la normalización de las predicciones\n",
    "    future_predictions_scaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "    # Obtener la fecha más reciente de los datos conocidos\n",
    "    last_date = original_data.index[-1]\n",
    "    \n",
    "    # Crear las fechas correspondientes para las predicciones\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta('1 hour'), periods=len(future_predictions_scaled), freq='H')\n",
    "\n",
    "    # Crear un DataFrame con las fechas y las predicciones escaladas\n",
    "    future_predictions_df = pd.DataFrame(future_predictions_scaled, columns=['Prediction'], index=future_dates)\n",
    "\n",
    "    # Actualizar el DataFrame original con la primera predicción\n",
    "    print(original_data.size)\n",
    "    original_data.loc[future_dates[0]] = future_predictions_scaled[0, 0]\n",
    "    print('='*50)\n",
    "    print(original_data.size)\n",
    "    \n",
    "    return future_predictions_df, original_data\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_future_steps = int(nan_sequences[x].get('num_nans'))\n",
    "print(num_future_steps)  # La cantidad de pasos que deseas predecir\n",
    "last_known_sequence = test_scaled[-lookback:].values\n",
    "print(data_river.size)\n",
    "future_predictions_df, updated_data = predict_future_autoregressive(model, last_known_sequence, num_future_steps, scaler, imputation_aitsu, device)\n",
    "\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Predicciones futuras:\")\n",
    "print(future_predictions_df)\n",
    "\n",
    "print(\"Datos originales actualizados con la primera predicción:\")\n",
    "print(updated_data.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_sequences = find_nan_sequences(data_river_2, max_consecutive_nans=120, view=True)\n",
    "nan_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n",
    "class ImprovedLSTM(nn.Module):\n",
    "       def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "           super(ImprovedLSTM, self).__init__()\n",
    "           self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "           self.fc = nn.Linear(hidden_size, 32)\n",
    "           self.relu = nn.ReLU()\n",
    "           self.dropout = nn.Dropout(dropout)\n",
    "           self.output = nn.Linear(32, output_horizon)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           out, _ = self.lstm(x)\n",
    "           out = self.fc(out[:, -1, :])\n",
    "           out = self.relu(out)\n",
    "           out = self.dropout(out)\n",
    "           out = self.output(out)\n",
    "           return out\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "output_horizon = 6\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device) \n",
    "\n",
    "# Función principal que realiza el proceso de predicción para completar los valores faltantes\n",
    "def fill_missing_values_with_lstm(data, nan_sequences, lookback, forecast_horizon, model, device, num_epochs=5000):\n",
    "    for x in range(len(nan_sequences)):\n",
    "        # Obtener la ventana de datos hasta el primer valor faltante\n",
    "        end_train = nan_sequences[x]['start_date'] - pd.Timedelta('1 hour 00:00:00')\n",
    "        data_river = data[:end_train]\n",
    "        print(f'Inicio del entrenamiento: {data_river.index[0]}, Fin del entrenamiento: {data_river.index[-1]}')\n",
    "\n",
    "        # Dividir los datos en train, val y test\n",
    "        total_size = len(data_river)\n",
    "        train_size = int(total_size * 0.70)\n",
    "        val_size = int(total_size * 0.15)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        train = data_river[:train_size]\n",
    "        val = data_river[train_size:train_size + val_size]\n",
    "        test = data_river[train_size + val_size:]\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler.fit(train.values)\n",
    "\n",
    "        # Normalizar los datos\n",
    "        scaler.fit(train.values)\n",
    "        train_scaled = scaler.transform(train.values)\n",
    "        val_scaled = scaler.transform(val.values)\n",
    "        test_scaled = scaler.transform(test.values)\n",
    "\n",
    "        # Crear las secuencias de entrenamiento, validación y test\n",
    "        x_train, y_train = create_sequences(train_scaled, lookback, forecast_horizon)\n",
    "        x_val, y_val = create_sequences(val_scaled, lookback, forecast_horizon)\n",
    "        x_test, y_test = create_sequences(test_scaled, lookback, forecast_horizon)\n",
    "\n",
    "        # Convertir los datos a tensores y moverlos a la GPU si es posible\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model = train_lstm_model(model, x_train, y_train, x_val, y_val, num_epochs, device)\n",
    "\n",
    "        # Obtener la última secuencia de datos conocida (última ventana del test set)\n",
    "        last_known_sequence = torch.tensor(test_scaled[-lookback:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Número de valores faltantes consecutivos\n",
    "        num_future_steps = int(nan_sequences[x].get('num_nans'))\n",
    "        print(f'Número de pasos futuros a predecir: {num_future_steps}')\n",
    "\n",
    "        # Realizar predicción autoregresiva\n",
    "        future_predictions_df, data = predict_future_autoregressive(model, last_known_sequence, num_future_steps, scaler, data, device)\n",
    "\n",
    "        print(f'Predicciones para el hueco {x+1}:')\n",
    "        print(future_predictions_df)\n",
    "\n",
    "    return data, scaler\n",
    "\n",
    "# Función para entrenar el modelo LSTM\n",
    "def train_lstm_model(model, x_train, y_train, x_val, y_val, num_epochs, device):\n",
    "    print(f\"Entrenando el modelo durante {num_epochs} épocas...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train = model(x_train)\n",
    "        loss = loss_fn(y_pred_train, y_train.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(x_val)\n",
    "            val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(best_model, 'best_lstm_model.pth')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Cargar el mejor modelo\n",
    "    print(f\"Mejor modelo guardado con pérdida de validación: {best_val_loss:.6f}\")\n",
    "    model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_future_autoregressive(model, last_known_sequence, data_miss, scaler, original_data, device):\n",
    "    print('Predicting future values...')\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    \n",
    "    # Convertir la secuencia conocida en un tensor de PyTorch\n",
    "    current_sequence = torch.tensor(last_known_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Revisar que current_sequence tenga las dimensiones correctas (batch_size, sequence_length, input_size)\n",
    "    if current_sequence.dim() == 4:\n",
    "        current_sequence = current_sequence.squeeze(0)  # Eliminar la dimensión extra si es 4D\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            # Generar la predicción\n",
    "            prediction = model(current_sequence.float())\n",
    "            \n",
    "            # Asegurarse de que prediction sea 3D antes de la concatenación\n",
    "            if prediction.dim() == 2:\n",
    "                prediction = prediction.unsqueeze(-1)  # Convertir a 3D (batch_size, output_horizon, input_size)\n",
    "\n",
    "            # Extraer la primera predicción y añadirla a las predicciones futuras\n",
    "            future_predictions.append(prediction[0, -1, 0].item())\n",
    "\n",
    "            # Actualizar la secuencia actual con la predicción generada\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    # Invertir la normalización de las predicciones\n",
    "    future_predictions_scaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "    # Obtener la fecha más reciente de los datos conocidos\n",
    "    last_date = original_data.index[-1]\n",
    "    \n",
    "    # Crear las fechas correspondientes para las predicciones\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta('1 hour'), periods=len(future_predictions_scaled), freq='H')\n",
    "\n",
    "    # Crear un DataFrame con las fechas y las predicciones escaladas\n",
    "    future_predictions_df = pd.DataFrame(future_predictions_scaled, columns=['Prediction'], index=future_dates)\n",
    "\n",
    "    # Actualizar el DataFrame original con la primera predicción\n",
    "    original_data.loc[future_dates[0]] = future_predictions_scaled[0, 0]\n",
    "    \n",
    "    return future_predictions_df, original_data\n",
    "\n",
    "# Ejemplo de uso\n",
    "nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=True)\n",
    "imputation_aitsu_filled, scaler = fill_missing_values_with_lstm(imputation_aitsu, nan_sequences, lookback=24, forecast_horizon=6, model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# %%\n",
    "# Definimos las funciones necesarias para el preprocesamiento y análisis de los datos\n",
    "def create_sequences(data, lookback=24, forecast_horizon=6):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        x = data[i:(i + lookback)]\n",
    "        y = data[(i + lookback):(i + lookback + forecast_horizon)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return torch.tensor(xs, dtype=torch.float32), torch.tensor(ys, dtype=torch.float32)\n",
    "\n",
    "def find_nan_sequences(df, max_consecutive_nans=100, view=False):\n",
    "    \"\"\"\n",
    "    Encuentra secuencias de valores faltantes en el DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando búsqueda de secuencias de valores NaN\")\n",
    "    nan_index = df[df.isnull().any(axis=1)].index\n",
    "    nan_sequences = []\n",
    "    \n",
    "    if len(nan_index) == 0:\n",
    "        logging.info(\"No se encontraron valores NaN en el DataFrame\")\n",
    "        return nan_sequences\n",
    "\n",
    "    start_date = nan_index[0]\n",
    "    prev_date = nan_index[0]\n",
    "    \n",
    "    for i in range(1, len(nan_index)):\n",
    "        current_date = nan_index[i]\n",
    "        \n",
    "        if current_date - prev_date != pd.Timedelta('1 hour 00:00:00'):\n",
    "            end_date = prev_date\n",
    "            num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "            \n",
    "            if num_nans == 1:\n",
    "                sequence_type = 'single'\n",
    "            elif num_nans <= max_consecutive_nans:\n",
    "                sequence_type = 'consecutive'\n",
    "            else:\n",
    "                sequence_type = 'long_consecutive'\n",
    "            \n",
    "            nan_sequences.append({\n",
    "                'type': sequence_type,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'num_nans': num_nans\n",
    "            })\n",
    "            \n",
    "            logging.info(f\"Secuencia encontrada: {sequence_type} - Inicio: {start_date}, Fin: {end_date}, Valores: {int(num_nans)}\")\n",
    "            start_date = current_date\n",
    "        \n",
    "        prev_date = current_date\n",
    "    \n",
    "    # Procesar la última secuencia\n",
    "    end_date = prev_date\n",
    "    num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "    sequence_type = 'single' if num_nans == 1 else 'consecutive' if num_nans <= max_consecutive_nans else 'long_consecutive'\n",
    "    \n",
    "    nan_sequences.append({\n",
    "        'type': sequence_type,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'num_nans': num_nans\n",
    "    })\n",
    "    \n",
    "    logging.info(f\"Total de secuencias encontradas: {len(nan_sequences)}\")\n",
    "    return nan_sequences\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # mse = ((y_true - y_pred) ** 2).mean()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, mape\n",
    "\n",
    "# %%\n",
    "# Definir el modelo LSTM mejorado\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "        self.fc = nn.Linear(hidden_size, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(32, output_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# %%\n",
    "# Función para entrenar el modelo con los datos disponibles\n",
    "def train_model(data_river, lookback=24, forecast_horizon=6):\n",
    "    logging.info(\"Iniciando entrenamiento del modelo\")\n",
    "    logging.info(f\"Parámetros - Lookback: {lookback}, Forecast Horizon: {forecast_horizon}\")\n",
    "    total_size = len(data_river)\n",
    "    train_size = int(total_size * 0.70)\n",
    "    val_size = int(total_size * 0.15)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    train = data_river[:train_size]\n",
    "    val = data_river[train_size:train_size + val_size]\n",
    "    test = data_river[train_size + val_size:]\n",
    "    logging.info(f\"División de datos - Train: {len(train)}, Validation: {len(val)}, Test: {len(test)}\")\n",
    "    \n",
    "    # Normalización\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(train.values)\n",
    "\n",
    "    # Transformar los conjuntos\n",
    "    train_scaled = scaler.transform(train.values)\n",
    "    val_scaled = scaler.transform(val.values)\n",
    "    test_scaled = scaler.transform(test.values)\n",
    "\n",
    "    # Crear secuencias\n",
    "    try:\n",
    "        x_train, y_train = create_sequences(train_scaled, lookback, forecast_horizon)\n",
    "        x_val, y_val = create_sequences(val_scaled, lookback, forecast_horizon)\n",
    "        x_test, y_test = create_sequences(test_scaled, lookback, forecast_horizon)\n",
    "        \n",
    "        logging.info(f\"Secuencias creadas - Train: {len(x_train)}, Val: {len(x_val)}, Test: {len(x_test)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al crear secuencias: {str(e)}\")\n",
    "        raise\n",
    "    print(f\"Longitud de x_train: {len(x_train)}\")\n",
    "    print(f\"Longitud de x_val:' {len(x_val)}\")\n",
    "    print(f\"Longitud de x_test:' {len(x_test)}\")\n",
    "\n",
    "    # Configuración del modelo y entrenamiento\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "    # Convertir a tensores\n",
    "    x_train = x_train.to(device).float()    \n",
    "    y_train = y_train.to(device).float()\n",
    "    x_val = x_val.to(device).float()\n",
    "    y_val = y_val.to(device).float()\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    input_size = 1\n",
    "    hidden_size = 128\n",
    "    num_layers = 1\n",
    "    dropout = 0.1\n",
    "\n",
    "    model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=forecast_horizon, dropout=dropout)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Parámetros de entrenamiento\n",
    "    weight_decay = 1e-5\n",
    "    lr = 1e-2\n",
    "    batch_size = 64\n",
    "    scheduler_patience = 15\n",
    "    early_stopping_patience = 20\n",
    "    n_epochs = 1000\n",
    "    validation_freq = 1\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=scheduler_patience, threshold=0.00001)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    val_rmse = []\n",
    "    val_maes = []\n",
    "    val_mapes = []\n",
    "    learning_rates = []\n",
    "    counter = 0\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range (n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        epoch_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        # Validación\n",
    "        if epoch % validation_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_val = model(x_val)\n",
    "                val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                y_true = scaler.inverse_transform(y_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "                y_pred_np = scaler.inverse_transform(y_pred_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "                rmse, mae, mape = calculate_metrics(y_true.flatten(), y_pred_np.flatten())\n",
    "                val_rmse.append(rmse)\n",
    "                val_maes.append(mae)\n",
    "                val_mapes.append(mape)\n",
    "\n",
    "        scheduler.step(val_loss.item())\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            counter = 0\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            torch.save(best_model, 'best_model_temp.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\nEntrenamiento completado en {total_time:.2f} segundos')\n",
    "    print(f'Tiempo promedio por época: {total_time/n_epochs:.2f} segundos')\n",
    "    print(f\"Best model was saved at epoch {best_epoch} with validation RMSE: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Cargar el mejor modelo\n",
    "    model.load_state_dict(torch.load('best_model_temp.pth'))\n",
    "\n",
    "    # save_images_train_model(epoch, train_losses, val_losses, val_rmse, val_maes, val_mapes, learning_rates)\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "# %%\n",
    "# Función para predecir los valores faltantes\n",
    "def predict_missing_values(model, scaler, imputation_aitsu, start_date, end_date, lookback=24, forecast_horizon=6):\n",
    "    logging.info(f\"Prediciendo valores faltantes desde {start_date} hasta {end_date}\")\n",
    "    \n",
    "    try:\n",
    "        data_before = imputation_aitsu[:start_date - pd.Timedelta('1 hour')]\n",
    "        input_data = data_before[-lookback:].values\n",
    "        input_scaled = scaler.transform(input_data)\n",
    "        \n",
    "        missing_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "        predicted_values = []\n",
    "        \n",
    "        x_input = torch.tensor(input_scaled.reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            while len(predicted_values) < len(missing_dates):\n",
    "                y_pred = model(x_input).cpu().numpy()\n",
    "                y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "                predicted_values.extend(y_pred_inv.flatten())\n",
    "                \n",
    "                last_values = np.concatenate((x_input.cpu().numpy().flatten()[forecast_horizon:], y_pred.flatten()))\n",
    "                x_input = torch.tensor(last_values[-lookback:].reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "        \n",
    "        predicted_values = predicted_values[:len(missing_dates)]\n",
    "        df_pred = pd.DataFrame(data=predicted_values, index=missing_dates, columns=[variable])\n",
    "        \n",
    "        logging.info(f\"Predicción completada: {len(predicted_values)} valores generados\")\n",
    "        return df_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en la predicción: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def setup_logging(variable_name):\n",
    "    \"\"\"\n",
    "    Configura el sistema de logging para el proceso de imputación.\n",
    "    \"\"\"\n",
    "    # Crear directorio para logs si no existe\n",
    "    log_dir = Path('logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Crear nombre del archivo de log\n",
    "    log_filename = log_dir / f'imputation_{variable_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "    \n",
    "    # Configurar el logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()  # Para mostrar también en consola\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Iniciando proceso de imputación para la variable: {variable_name}\")\n",
    "    logging.info(f\"Archivo de log creado en: {log_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Código principal\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        variable = 'LLUVIA_Aitzu-Urola'\n",
    "        setup_logging(variable)\n",
    "        logging.info(\"Cargando datos...\")\n",
    "        data_river_2 = pd.read_csv('db_21.csv')\n",
    "        data_river_2 = data_river_2.set_index('Fecha')\n",
    "        data_river_2.index = pd.to_datetime(data_river_2.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        imputation_aitsu = data_river_2[[variable]].copy()\n",
    "        date_init = '1999-03-16 17:00'\n",
    "        date_end = '2023-09-30 23:00'\n",
    "        imputation_aitsu = imputation_aitsu[date_init:date_end] \n",
    "\n",
    "        logging.info(f\"Datos cargados: {len(imputation_aitsu)} registros\")\n",
    "        # %%\n",
    "        # Encontrar las secuencias de valores faltantes\n",
    "        nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=True)\n",
    "\n",
    "        # %%\n",
    "        # Proceso iterativo para completar los valores faltantes\n",
    "        lookback = 24\n",
    "        forecast_horizon = 6\n",
    "\n",
    "        start_train_date = imputation_aitsu.index[0]\n",
    "        for seq in nan_sequences:\n",
    "            print(seq)\n",
    "            if seq['type'] == 'long_consecutive':\n",
    "                logging.warning(f\"Secuencia muy larga detectada: {seq['start_date']} a {seq['end_date']}, {int(seq['num_nans'])} valores\")\n",
    "                start_train_date = seq['end_date'] + pd.Timedelta('1 hour')\n",
    "            else:\n",
    "                start_date = seq['start_date']\n",
    "                end_date = seq['end_date']\n",
    "                num_nans = seq['num_nans']\n",
    "                logging.info(f\"Procesando secuencia: {start_date} a {end_date} ({int(num_nans)} valores)\")\n",
    "\n",
    "                # Obtener los datos hasta la secuencia faltante\n",
    "                data_river = imputation_aitsu[start_train_date:start_date - pd.Timedelta('1 hour')]\n",
    "                print(\"start_train_date: \", start_train_date)\n",
    "                print(\"start_date: \", start_date)\n",
    "                print(data_river.index[0])\n",
    "\n",
    "                # Verificar si hay suficientes datos para entrenar\n",
    "                if len(data_river) < lookback + forecast_horizon:\n",
    "                    logging.warning(f\"Datos insuficientes para entrenar antes de {start_date}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Entrenando modelo para predecir valores faltantes desde {start_date} hasta {end_date}: ({int(num_nans)} valores faltantes)\")\n",
    "\n",
    "                # Entrenar el modelo\n",
    "                model, scaler = train_model(data_river, lookback, forecast_horizon)\n",
    "\n",
    "                # Predecir los valores faltantes\n",
    "                df_pred = predict_missing_values(model, scaler, imputation_aitsu, start_date, end_date, lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "\n",
    "                # Actualizar el DataFrame principal con los valores predichos\n",
    "                imputation_aitsu.update(df_pred)\n",
    "\n",
    "# %%\n",
    "# Verificar si quedan valores faltantes\n",
    "        remaining_nans = imputation_aitsu.isna().sum()\n",
    "        logging.info(f\"Proceso completado. Valores faltantes restantes: {remaining_nans}\")\n",
    "\n",
    "# %%\n",
    "# Guardar el DataFrame con los valores completados\n",
    "        output_file = f'imputation_caudal{variable}_completed.csv'\n",
    "        imputation_aitsu.to_csv(output_file)\n",
    "        logging.info(f\"Resultados guardados en: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso principal: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Opcional: graficar los datos originales y los datos completados\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(imputation_aitsu.index, imputation_aitsu[variable], label='Datos con imputación', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Datos con valores imputados')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel del Río')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urola_Data = pd.read_csv(\"imputation_CAUDAL_RIO_Ibaieder_completed.csv\")\n",
    "urola_Data = urola_Data.set_index('Fecha')\n",
    "urola_Data.index = pd.to_datetime(urola_Data.index, format='%Y-%m-%d %H:%M:%S')\n",
    "urola_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"2021-10-13\"\n",
    "end = \"2021-11-02\"\n",
    "plt.plot(urola_Data[init:end][variable])\n",
    "plt.plot(data_river_2[init:end][variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(urola_Data.isna().sum())\n",
    "nan_sequences1 = find_nan_sequences(urola_Data, max_consecutive_nans=120, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_date_range(model_path, scaler, imputation_aitsu, start_date, end_date, lookback=100, forecast_horizon=6):\n",
    "    class ImprovedLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "            super(ImprovedLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_size, 32)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.output = nn.Linear(32, output_horizon)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.output(out)\n",
    "            return out\n",
    "\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Load the saved model\n",
    "    model = ImprovedLSTM(input_size=1, hidden_size=256, num_layers=3, output_horizon=forecast_horizon, dropout=0.2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare data up to the prediction start date\n",
    "    data_before = imputation_aitsu[:start_date - pd.Timedelta('1 hour')]\n",
    "    input_data = data_before[-lookback:].values\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "\n",
    "    # Set up the input tensor for prediction\n",
    "    missing_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    predicted_values = []\n",
    "    x_input = torch.tensor(input_scaled.reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        while len(predicted_values) < len(missing_dates):\n",
    "            y_pred = model(x_input).cpu().numpy()\n",
    "            y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "            predicted_values.extend(y_pred_inv.flatten())\n",
    "\n",
    "            # Update input for the next prediction\n",
    "            last_values = np.concatenate((x_input.cpu().numpy().flatten()[forecast_horizon:], y_pred.flatten()))\n",
    "            x_input = torch.tensor(last_values[-lookback:].reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "    predicted_values = predicted_values[:len(missing_dates)]\n",
    "    df_pred = pd.DataFrame(data=predicted_values, index=missing_dates, columns=[variable])\n",
    "\n",
    "    return df_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2021-10-13'\n",
    "end_date = '2021-11-02'\n",
    "df_pred = predict_date_range('best_model_temp.pth', scaler, urola_Data, start_date, end_date)\n",
    "print(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(urola_Data[start_date:end_date][variable], label='Datos reales', alpha=0.7)\n",
    "plt.plot(df_pred[variable], label='Predicciones', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Predicción de valores faltantes')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel del Río')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4flood_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
