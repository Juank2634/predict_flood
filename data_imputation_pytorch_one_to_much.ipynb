{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install plotly\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las siguientes funciones para el preprocesamiento de los datos\n",
    "#Con la siguiente funciona nos aseguramos que los datos de entrada tengan el formato correcto para el funcionamiento de las redes recurrentes \n",
    "def create_sequences(data, lookback=24, forecast_horizon=6):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        x = data[i:(i + lookback)]\n",
    "        y = data[(i + lookback):(i + lookback + forecast_horizon)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return torch.tensor(xs, dtype=torch.float32), torch.tensor(ys, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#Con la siguiente funcion analizamos los datos duplicados en el indice del dataframe \n",
    "def analyze_duplicate_indices(df):\n",
    "    # Identificar índices duplicados\n",
    "    duplicated_indices = df.index[df.index.duplicated(keep=False)]\n",
    "    \n",
    "    if duplicated_indices.empty:\n",
    "        print(\"No se encontraron índices duplicados.\")\n",
    "        return None\n",
    "\n",
    "    # Ordenar los duplicados\n",
    "    duplicates_sorted = df.loc[duplicated_indices].sort_index()\n",
    "\n",
    "    # Agrupar los duplicados\n",
    "    grouped_duplicates = duplicates_sorted.groupby(duplicates_sorted.index)\n",
    "\n",
    "    duplicate_info = []\n",
    "\n",
    "    for idx, group in grouped_duplicates:\n",
    "        duplicate_info.append({\n",
    "            'index': idx,\n",
    "            'count': len(group),\n",
    "            'values': group.values.flatten().tolist()\n",
    "        })\n",
    "\n",
    "    return duplicate_info\n",
    "\n",
    "def remove_duplicate_indices(df, method='first'):\n",
    "    # Analizar duplicados\n",
    "    duplicate_info = analyze_duplicate_indices(df)\n",
    "    \n",
    "    if not duplicate_info:\n",
    "        print(\"No hay índices duplicados para eliminar.\")\n",
    "        return df\n",
    "    \n",
    "    # Eliminar duplicados\n",
    "    df_sin_duplicados = df[~df.index.duplicated(keep=method)]\n",
    "    \n",
    "    print(f\"Se eliminaron {len(df) - len(df_sin_duplicados)} filas con índices duplicados.\")\n",
    "    print(f\"Tamaño original del DataFrame: {len(df)}\")\n",
    "    print(f\"Tamaño del DataFrame sin duplicados: {len(df_sin_duplicados)}\")\n",
    "    \n",
    "    return df_sin_duplicados\n",
    "\n",
    "def find_nan_sequences(df, max_consecutive_nans=120, view=False):\n",
    "    nan_index = df[df.isnull().any(axis=1)].index\n",
    "    # print(nan_index)\n",
    "    nan_sequences = []\n",
    "    \n",
    "    if len(nan_index) == 0:\n",
    "        return nan_sequences\n",
    "\n",
    "    start_date = nan_index[0]\n",
    "    prev_date = nan_index[0]\n",
    "    \n",
    "    for i in range(1, len(nan_index)):\n",
    "        current_date = nan_index[i]\n",
    "        \n",
    "        if current_date - prev_date != pd.Timedelta('1 hour 00:00:00'):\n",
    "            # No es consecutivo, guardar la secuencia anterior\n",
    "            end_date = prev_date\n",
    "            num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "            \n",
    "            if num_nans == 1:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'single',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            elif num_nans <= max_consecutive_nans:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'consecutive',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            elif num_nans > max_consecutive_nans:\n",
    "                nan_sequences.append({\n",
    "                    'type': 'long_consecutive',\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'num_nans': num_nans\n",
    "                })\n",
    "            else:\n",
    "                if view:\n",
    "                    print(f'Secuencia de NaN mayor a {max_consecutive_nans} horas: {start_date} a {end_date}, {num_nans} valores')\n",
    "            \n",
    "            # Iniciar una nueva secuencia\n",
    "            start_date = current_date\n",
    "        \n",
    "        prev_date = current_date\n",
    "    \n",
    "    # Manejar la última secuencia\n",
    "    end_date = prev_date\n",
    "    num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "    \n",
    "    if num_nans == 1:\n",
    "        nan_sequences.append({\n",
    "            'type': 'single',\n",
    "            'date': start_date\n",
    "        })\n",
    "    elif num_nans <= max_consecutive_nans:\n",
    "        nan_sequences.append({\n",
    "            'type': 'consecutive',\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'num_nans': num_nans\n",
    "        })\n",
    "    else:\n",
    "        if view:\n",
    "            print(f'Secuencia de NaN mayor a {max_consecutive_nans} horas: {start_date} a {end_date}, {num_nans} valores')\n",
    "    \n",
    "    return nan_sequences\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la variable imputation_lluvia_aitsu en otra variable para las modificaciones\n",
    "variable = 'CAUDAL_RIO_Aitzu-Urola'\n",
    "data_river_2 = pd.read_csv('db_21.csv')\n",
    "data_river_2 = data_river_2.set_index('Fecha')\n",
    "# Asignar un formato de fecha a la columna fecha\n",
    "data_river_2.index = pd.to_datetime(data_river_2.index, format='%Y-%m-%d %H:%M:%S')\n",
    "# Se trabajara sobre la variable de nivel del rio Aitzu-Urola \n",
    "imputation_aitsu = data_river_2[[variable]].copy()\n",
    "date_init = '1999-03-16 17:00'\n",
    "date_end = '2023-09-30 23:00'\n",
    "imputation_aitsu = imputation_aitsu[date_init:date_end] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imputation_aitsu.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aitzu_image = data_river_2.copy()\n",
    "plt.figure(figsize=(100, 40))\n",
    "plt.plot(list(aitzu_image['CAUDAL_RIO_Aitzu-Urola']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función\n",
    "# duplicate_info = analyze_duplicate_indices(imputation_lluvia_aitsu)\n",
    "\n",
    "# Usar la función\n",
    "# imputation_lluvia_aitsu_sin_duplicados = remove_duplicate_indices(imputation_lluvia_aitsu, method='first')\n",
    "\n",
    "# Uso de la función\n",
    "nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=True)\n",
    "\n",
    "# Imprimir resultados\n",
    "for x in nan_sequences:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar la secuencia de datos para el entrenamiento\n",
    "x = 0\n",
    "# init_train = nan_sequences[0]['end_date'] + pd.Timedelta('1 hour 00:00:00')\n",
    "end_train = nan_sequences[x]['start_date'] - pd.Timedelta('1 hour 00:00:00')\n",
    "data_river = imputation_aitsu[:end_train]\n",
    "print('Fecha de inicio para el entrenamiento ----> ', data_river.index[0])\n",
    "print('Fecha de fin del entrenamiento ------> ', data_river.index[-1])\n",
    "\n",
    "total_size = len(data_river)\n",
    "train_size = int(total_size * 0.70)\n",
    "val_size = int(total_size * 0.15)\n",
    "test_size = total_size - train_size - val_size\n",
    "train = data_river[:train_size]\n",
    "val = data_river[train_size:train_size + val_size]\n",
    "test = data_river[train_size + val_size:]\n",
    "\n",
    "print('Numero de valores para el train:{}, val:{} y test:{}'.format(train.shape, val.shape, test.shape))\n",
    "print(data_river.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion de los datos\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar los conjuntos\n",
    "train_scaled = scaler.transform(train.values)\n",
    "val_scaled = scaler.transform(val.values)\n",
    "test_scaled = scaler.transform(test.values)\n",
    "\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=[variable], index=train.index)\n",
    "val_scaled = pd.DataFrame(val_scaled, columns=[variable], index=val.index)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=[variable], index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las secuencias de entrenamiento, validación y test\n",
    "lookback = 24\n",
    "forecast_horizon = 6\n",
    "x_train, y_train = create_sequences(train_scaled.values, lookback, forecast_horizon)\n",
    "x_val, y_val = create_sequences(val_scaled.values, lookback, forecast_horizon)\n",
    "x_test, y_test = create_sequences(test_scaled.values, lookback, forecast_horizon)\n",
    "print('Tamaño de las secuencias:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
    "#         self.linear = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "\n",
    "# class ImprovedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size = 1, hidden_size = 64, num_layers = 2, dropout = 0.2):\n",
    "#         super(ImprovedLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out.unsqueeze(1)\n",
    "    \n",
    "class ImprovedLSTM(nn.Module):\n",
    "       def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "           super(ImprovedLSTM, self).__init__()\n",
    "           self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "           self.fc = nn.Linear(hidden_size, 32)\n",
    "           self.relu = nn.ReLU()\n",
    "           self.dropout = nn.Dropout(dropout)\n",
    "           self.output = nn.Linear(32, output_horizon)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           out, _ = self.lstm(x)\n",
    "           out = self.fc(out[:, -1, :])\n",
    "           out = self.relu(out)\n",
    "           out = self.dropout(out)\n",
    "           out = self.output(out)\n",
    "           return out\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_horizon = forecast_horizon\n",
    "dropout = 0.2\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# class ImprovedModel(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "#                             num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.linear(out[:, -1, :])  # Toma solo el último paso temporal\n",
    "#         return out.unsqueeze(1)\n",
    "\n",
    "# model = ImprovedLSTM(input_size=1, hidden_size=64, num_layers=2, dropout=0.2)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "weight_decay = 1e-5\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "scheduler_patience = 10\n",
    "early_stopping_patience = 20\n",
    "n_epochs = 1000\n",
    "validation_freq = 1\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "output_horizon = forecast_horizon\n",
    "dropout = 0.3\n",
    "\n",
    "x_train = x_train.to(device).float()    \n",
    "y_train = y_train.to(device).float()\n",
    "x_val = x_val.to(device).float()\n",
    "y_val = y_val.to(device).float()\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=scheduler_patience, threshold=0.00001)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "val_rmse = []\n",
    "val_maes = []\n",
    "val_mapes = []\n",
    "learning_rates = []\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_epoch = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pred_train = []\n",
    "pred_val = []\n",
    "pred_test = []\n",
    "\n",
    "for epoch in range (n_epochs):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)  # Add an underscore to ignore the hidden state\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        epoch_train_losses.append(loss.item())\n",
    "    \n",
    "    epoch_train_loss = np.mean(epoch_train_losses)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    #Validacion\n",
    "    if epoch % validation_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(x_val)\n",
    "            val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # Corregir la forma de los datos antes de aplicar inverse_transform\n",
    "            y_true = scaler.inverse_transform(y_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "            y_pred_np = scaler.inverse_transform(y_pred_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "            rmse, mae, mape = calculate_metrics(y_true.flatten(), y_pred_np.flatten())\n",
    "            val_rmse.append(rmse)\n",
    "            val_maes.append(mae)\n",
    "            val_mapes.append(mape)\n",
    "\n",
    "    scheduler.step(val_loss.item())\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {epoch_train_loss:.6f}, Val Loss: {val_loss.item():.6f}, Val RMSE: {rmse:.6f}, Val MAE: {mae:.6f}, Val MAPE: {mape:.6f}, LR: {current_lr:.8f}, # Epoch total: {n_epochs}')\n",
    "        \n",
    "        # Early stopping\n",
    "    if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            counter = 0\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            torch.save(best_model, 'best_model_aitsu.pth')\n",
    "    else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nEntrenamiento completado en {total_time:.2f} segundos')\n",
    "print(f'Tiempo promedio por época: {total_time/n_epochs:.2f} segundos')\n",
    "print(f\"Best model was saved at epoch {best_epoch} with validation RMSE: {best_val_loss:.4f}\")\n",
    "\n",
    "# Evaluacion en el conjunto de test\n",
    "# Evaluación en el conjunto de test\n",
    "model.load_state_dict(torch.load('best_model_aitsu.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(x_test.to(device).float())\n",
    "    y_true_test = scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "    y_pred_test_np = scaler.inverse_transform(y_pred_test.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "    rmse_test, mae_test, mape_test = calculate_metrics(y_true_test.flatten(), y_pred_test_np.flatten())\n",
    "    print(f'Test RMSE: {rmse_test:.6f}, Test MAE: {mae_test:.6f}, Test MAPE: {mape_test:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(0, n_epochs, 1))  # Asumiendo que guardamos métricas cada 100 épocas\n",
    "train_losses_plot = train_losses  # Tomamos cada 100º elemento\n",
    "val_losses_plot = val_losses\n",
    "val_rmses_plot = val_rmse\n",
    "learning_rates_plot = learning_rates\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, \n",
    "                    subplot_titles=('Loss', 'Metrics', 'Learning Rate', 'predictions'),\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "# Añadir pérdidas de entrenamiento y validación\n",
    "fig.add_trace(go.Scatter(x=epochs, y=train_losses_plot, mode='lines', name='Training Loss',\n",
    "                         line=dict(color='blue', width=2)),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_losses_plot, mode='lines', name='Validation Loss',\n",
    "                         line=dict(color='red', width=2)),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Añadir RMSE, MAE y MAPE\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_rmses_plot, mode='lines', name='RMSE',\n",
    "                         line=dict(color='green', width=2)),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_maes, mode='lines', name='MAE',\n",
    "                         line=dict(color='orange', width=2)),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_mapes, mode='lines', name='MAPE',\n",
    "                         line=dict(color='purple', width=2)),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Añadir tasa de aprendizaje\n",
    "fig.add_trace(go.Scatter(x=epochs, y=learning_rates_plot, mode='lines', name='Learning Rate',\n",
    "                         line=dict(color='brown', width=2)),\n",
    "              row=3, col=1)\n",
    "\n",
    "# # Predicciones de train y val\n",
    "# fig.add_trace(go.Scatter(x=train.index, y=np.array(pred_train).flatten(), mode='lines', name='Train Predictions',\n",
    "#                          line=dict(color='blue', width=2)),\n",
    "#               row=4, col=1)\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=val.index, y=np.array(pred_val).flatten(), mode='lines', name='Val Predictions',\n",
    "#                          line=dict(color='red', width=2)),\n",
    "#               row=4, col=1)\n",
    "\n",
    "\n",
    "# Actualizar el diseño\n",
    "fig.update_layout(height=1200, width=1400, title_text=\"Model Training Metrics\")\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Metric Value\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Learning Rate\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Predictions\", row=4, col=1)\n",
    "\n",
    "# Mostrar la figura\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los arrays para el plot\n",
    "data_river_scaled = pd.concat([train_scaled, val_scaled, test_scaled])\n",
    "data_river_scaled_values = data_river_scaled.values  # Para mayor claridad\n",
    "total_length = len(data_river_scaled_values)\n",
    "train_plot = np.ones((total_length, 1)) * np.nan\n",
    "val_plot = np.ones((total_length, 1)) * np.nan\n",
    "test_plot = np.ones((total_length, 1)) * np.nan\n",
    "\n",
    "# Predicciones para el conjunto de entrenamiento\n",
    "y_pred_train = model(x_train).detach().cpu().numpy()\n",
    "y_pred_train = y_pred_train.reshape(-1, forecast_horizon)\n",
    "start_idx = lookback + forecast_horizon - 1\n",
    "end_idx = start_idx + len(y_pred_train)\n",
    "train_plot[start_idx:end_idx] = y_pred_train[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Predicciones para el conjunto de validación\n",
    "y_pred_val = model(x_val.to(device)).detach().cpu().numpy()\n",
    "y_pred_val = y_pred_val.reshape(-1, forecast_horizon)\n",
    "val_start_idx = train_size + lookback + forecast_horizon - 1\n",
    "val_end_idx = val_start_idx + len(y_pred_val)\n",
    "val_plot[val_start_idx:val_end_idx] = y_pred_val[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Predicciones para el conjunto de test\n",
    "y_pred_test = model(x_test.to(device)).detach().cpu().numpy()\n",
    "y_pred_test = y_pred_test.reshape(-1, forecast_horizon)\n",
    "test_start_idx = train_size + val_size + lookback + forecast_horizon - 1\n",
    "test_end_idx = test_start_idx + len(y_pred_test)\n",
    "test_plot[test_start_idx:test_end_idx] = y_pred_test[:, -1].reshape(-1, 1)\n",
    "\n",
    "# Invertir la normalización\n",
    "train_plot = scaler.inverse_transform(train_plot)\n",
    "val_plot = scaler.inverse_transform(val_plot)\n",
    "test_plot = scaler.inverse_transform(test_plot)\n",
    "true_values = scaler.inverse_transform(data_river_scaled_values)\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(data_river_scaled.index, true_values, label='True', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, train_plot, label='Train Predictions', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, val_plot, label='Validation Predictions', alpha=0.7)\n",
    "plt.plot(data_river_scaled.index, test_plot, label='Test Predictions', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Model Predictions vs True Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('River Level')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade nbformat\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Preparar los arrays para el plot\n",
    "    data_river_scaled = pd.concat([train_scaled, val_scaled, test_scaled])\n",
    "    total_length = len(data_river_scaled)\n",
    "    train_plot = np.ones((total_length, 1)) * np.nan\n",
    "    val_plot = np.ones((total_length, 1)) * np.nan\n",
    "    test_plot = np.ones((total_length, 1)) * np.nan\n",
    "\n",
    "    # Predicciones para el conjunto de entrenamiento\n",
    "    x_train_gpu = x_train.to(device)\n",
    "    y_pred_train = model(x_train_gpu).cpu().numpy()\n",
    "    y_pred_train_last = y_pred_train[:, -1]\n",
    "    train_start_idx = lookback\n",
    "    train_end_idx = train_start_idx + len(y_pred_train_last)\n",
    "    train_plot[train_start_idx:train_end_idx] = y_pred_train_last.reshape(-1, 1)\n",
    "\n",
    "    # Predicciones para el conjunto de validación\n",
    "    x_val_gpu = x_val.to(device)\n",
    "    y_pred_val = model(x_val_gpu).cpu().numpy()\n",
    "    y_pred_val_last = y_pred_val[:, -1]\n",
    "    val_start_idx = train_size + lookback\n",
    "    val_end_idx = val_start_idx + len(y_pred_val_last)\n",
    "    val_plot[val_start_idx:val_end_idx] = y_pred_val_last.reshape(-1, 1)\n",
    "\n",
    "    # Predicciones para el conjunto de test\n",
    "    x_test_gpu = x_test.to(device)\n",
    "    y_pred_test = model(x_test_gpu).cpu().numpy()\n",
    "    y_pred_test_last = y_pred_test[:, -1]\n",
    "    test_start_idx = train_size + val_size + lookback\n",
    "    test_end_idx = test_start_idx + len(y_pred_test_last)\n",
    "    test_plot[test_start_idx:test_end_idx] = y_pred_test_last.reshape(-1, 1)\n",
    "\n",
    "    # Invertir la normalización\n",
    "    train_plot = scaler.inverse_transform(train_plot)\n",
    "    val_plot = scaler.inverse_transform(val_plot)\n",
    "    test_plot = scaler.inverse_transform(test_plot)\n",
    "    true_values = scaler.inverse_transform(data_river_scaled.values)\n",
    "\n",
    "\n",
    "# Crear el gráfico con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Añadir los datos reales\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=true_values.flatten(),\n",
    "                         mode='lines', name='Datos Reales', line=dict(color='blue', width=2)))\n",
    "\n",
    "# Añadir las predicciones de entrenamiento\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=train_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Train', line=dict(color='red', width=2)))\n",
    "\n",
    "# Añadir las predicciones de validación\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=val_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Val', line=dict(color='green', width=2)))\n",
    "\n",
    "# Añadir las predicciones de test\n",
    "fig.add_trace(go.Scatter(x=data_river_scaled.index, y=test_plot.flatten(),\n",
    "                         mode='lines', name='Predicciones Test', line=dict(color='purple', width=2)))\n",
    "\n",
    "# Configurar el diseño del gráfico\n",
    "fig.update_layout(\n",
    "    title='Predicciones del Modelo vs Valores Reales',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Nivel del Río',\n",
    "    legend_title='Leyenda',\n",
    "    width=2600,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para entrenamiento\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    train_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = train_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_train) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    train_plot_step[start_idx:start_idx + max_length] = y_pred_train[:max_length, step].reshape(-1, 1)\n",
    "    train_plot_step = scaler.inverse_transform(train_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=train_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Train Step {step + 1}',\n",
    "        line=dict(width=1),\n",
    "        opacity=0.8\n",
    "    ))\n",
    "\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para validación\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    val_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = val_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_val) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    val_plot_step[start_idx:start_idx + max_length] = y_pred_val[:max_length, step].reshape(-1, 1)\n",
    "    val_plot_step = scaler.inverse_transform(val_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=val_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Val Step {step + 1}',\n",
    "        line=dict(width=1, dash='dash'),\n",
    "        opacity=0.85\n",
    "    ))\n",
    "\n",
    "# Añadir las predicciones de cada paso en el horizonte para test\n",
    "for step in range(forecast_horizon):\n",
    "    offset = step\n",
    "    test_plot_step = np.ones((total_length, 1)) * np.nan\n",
    "    start_idx = test_start_idx + offset\n",
    "    # Calcular la longitud máxima que podemos asignar sin exceder los límites\n",
    "    max_length = min(len(y_pred_test) - offset, total_length - start_idx)\n",
    "    if max_length <= 0:\n",
    "        continue\n",
    "    test_plot_step[start_idx:start_idx + max_length] = y_pred_test[:max_length, step].reshape(-1, 1)\n",
    "    test_plot_step = scaler.inverse_transform(test_plot_step)\n",
    "    # Añadir la predicción al gráfico\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data_river_scaled.index,\n",
    "        y=test_plot_step.flatten(),\n",
    "        mode='lines',\n",
    "        name=f'Predicción Test Step {step + 1}',\n",
    "        line=dict(width=1, dash='dot'),\n",
    "        opacity=0.9\n",
    "    ))\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, last_known_sequence, data_miss):\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    current_sequence = last_known_sequence.clone().to(device)\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            prediction = model(current_sequence.float())\n",
    "            future_predictions.append(prediction[0, -1].item())\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    return future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future_autoregressive(model, last_known_sequence, data_miss, scaler, original_data, device):\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    # Convertir la secuencia conocida en un tensor de PyTorch\n",
    "    current_sequence = torch.tensor(last_known_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            # Generar la predicción\n",
    "            prediction = model(current_sequence.float())\n",
    "            prediction = prediction.unsqueeze(-1)  # Aseguramos que prediction sea 3D\n",
    "\n",
    "            # Extraer la primera predicción y añadirla a las predicciones futuras\n",
    "            future_predictions.append(prediction[0, -1, 0].item())\n",
    "\n",
    "            # Actualizar la secuencia actual con la predicción generada\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    # Invertir la normalización de las predicciones\n",
    "    future_predictions_scaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "    # Obtener la fecha más reciente de los datos conocidos\n",
    "    last_date = original_data.index[-1]\n",
    "    \n",
    "    # Crear las fechas correspondientes para las predicciones\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta('1 hour'), periods=len(future_predictions_scaled), freq='H')\n",
    "\n",
    "    # Crear un DataFrame con las fechas y las predicciones escaladas\n",
    "    future_predictions_df = pd.DataFrame(future_predictions_scaled, columns=['Prediction'], index=future_dates)\n",
    "\n",
    "    # Actualizar el DataFrame original con la primera predicción\n",
    "    print(original_data.size)\n",
    "    original_data.loc[future_dates[0]] = future_predictions_scaled[0, 0]\n",
    "    print('='*50)\n",
    "    print(original_data.size)\n",
    "    \n",
    "    return future_predictions_df, original_data\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_future_steps = int(nan_sequences[x].get('num_nans'))\n",
    "print(num_future_steps)  # La cantidad de pasos que deseas predecir\n",
    "last_known_sequence = test_scaled[-lookback:].values\n",
    "print(data_river.size)\n",
    "future_predictions_df, updated_data = predict_future_autoregressive(model, last_known_sequence, num_future_steps, scaler, imputation_aitsu, device)\n",
    "\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Predicciones futuras:\")\n",
    "print(future_predictions_df)\n",
    "\n",
    "print(\"Datos originales actualizados con la primera predicción:\")\n",
    "print(updated_data.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_sequences = find_nan_sequences(data_river_2, max_consecutive_nans=120, view=True)\n",
    "nan_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n",
    "class ImprovedLSTM(nn.Module):\n",
    "       def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "           super(ImprovedLSTM, self).__init__()\n",
    "           self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "           self.fc = nn.Linear(hidden_size, 32)\n",
    "           self.relu = nn.ReLU()\n",
    "           self.dropout = nn.Dropout(dropout)\n",
    "           self.output = nn.Linear(32, output_horizon)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           out, _ = self.lstm(x)\n",
    "           out = self.fc(out[:, -1, :])\n",
    "           out = self.relu(out)\n",
    "           out = self.dropout(out)\n",
    "           out = self.output(out)\n",
    "           return out\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "output_horizon = 6\n",
    "\n",
    "model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=output_horizon, dropout=dropout)\n",
    "model = model.to(device) \n",
    "\n",
    "# Función principal que realiza el proceso de predicción para completar los valores faltantes\n",
    "def fill_missing_values_with_lstm(data, nan_sequences, lookback, forecast_horizon, model, device, num_epochs=5000):\n",
    "    for x in range(len(nan_sequences)):\n",
    "        # Obtener la ventana de datos hasta el primer valor faltante\n",
    "        end_train = nan_sequences[x]['start_date'] - pd.Timedelta('1 hour 00:00:00')\n",
    "        data_river = data[:end_train]\n",
    "        print(f'Inicio del entrenamiento: {data_river.index[0]}, Fin del entrenamiento: {data_river.index[-1]}')\n",
    "\n",
    "        # Dividir los datos en train, val y test\n",
    "        total_size = len(data_river)\n",
    "        train_size = int(total_size * 0.70)\n",
    "        val_size = int(total_size * 0.15)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        train = data_river[:train_size]\n",
    "        val = data_river[train_size:train_size + val_size]\n",
    "        test = data_river[train_size + val_size:]\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler.fit(train.values)\n",
    "\n",
    "        # Normalizar los datos\n",
    "        scaler.fit(train.values)\n",
    "        train_scaled = scaler.transform(train.values)\n",
    "        val_scaled = scaler.transform(val.values)\n",
    "        test_scaled = scaler.transform(test.values)\n",
    "\n",
    "        # Crear las secuencias de entrenamiento, validación y test\n",
    "        x_train, y_train = create_sequences(train_scaled, lookback, forecast_horizon)\n",
    "        x_val, y_val = create_sequences(val_scaled, lookback, forecast_horizon)\n",
    "        x_test, y_test = create_sequences(test_scaled, lookback, forecast_horizon)\n",
    "\n",
    "        # Convertir los datos a tensores y moverlos a la GPU si es posible\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        model = train_lstm_model(model, x_train, y_train, x_val, y_val, num_epochs, device)\n",
    "\n",
    "        # Obtener la última secuencia de datos conocida (última ventana del test set)\n",
    "        last_known_sequence = torch.tensor(test_scaled[-lookback:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Número de valores faltantes consecutivos\n",
    "        num_future_steps = int(nan_sequences[x].get('num_nans'))\n",
    "        print(f'Número de pasos futuros a predecir: {num_future_steps}')\n",
    "\n",
    "        # Realizar predicción autoregresiva\n",
    "        future_predictions_df, data = predict_future_autoregressive(model, last_known_sequence, num_future_steps, scaler, data, device)\n",
    "\n",
    "        print(f'Predicciones para el hueco {x+1}:')\n",
    "        print(future_predictions_df)\n",
    "\n",
    "    return data, scaler\n",
    "\n",
    "# Función para entrenar el modelo LSTM\n",
    "def train_lstm_model(model, x_train, y_train, x_val, y_val, num_epochs, device):\n",
    "    print(f\"Entrenando el modelo durante {num_epochs} épocas...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train = model(x_train)\n",
    "        loss = loss_fn(y_pred_train, y_train.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(x_val)\n",
    "            val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(best_model, 'best_lstm_model.pth')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Cargar el mejor modelo\n",
    "    print(f\"Mejor modelo guardado con pérdida de validación: {best_val_loss:.6f}\")\n",
    "    model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_future_autoregressive(model, last_known_sequence, data_miss, scaler, original_data, device):\n",
    "    print('Predicting future values...')\n",
    "    model.eval()\n",
    "    future_predictions = []\n",
    "    \n",
    "    # Convertir la secuencia conocida en un tensor de PyTorch\n",
    "    current_sequence = torch.tensor(last_known_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Revisar que current_sequence tenga las dimensiones correctas (batch_size, sequence_length, input_size)\n",
    "    if current_sequence.dim() == 4:\n",
    "        current_sequence = current_sequence.squeeze(0)  # Eliminar la dimensión extra si es 4D\n",
    "\n",
    "    for _ in range(data_miss):\n",
    "        with torch.no_grad():\n",
    "            # Generar la predicción\n",
    "            prediction = model(current_sequence.float())\n",
    "            \n",
    "            # Asegurarse de que prediction sea 3D antes de la concatenación\n",
    "            if prediction.dim() == 2:\n",
    "                prediction = prediction.unsqueeze(-1)  # Convertir a 3D (batch_size, output_horizon, input_size)\n",
    "\n",
    "            # Extraer la primera predicción y añadirla a las predicciones futuras\n",
    "            future_predictions.append(prediction[0, -1, 0].item())\n",
    "\n",
    "            # Actualizar la secuencia actual con la predicción generada\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], prediction[:, -1:, :]), dim=1)\n",
    "\n",
    "    # Invertir la normalización de las predicciones\n",
    "    future_predictions_scaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
    "\n",
    "    # Obtener la fecha más reciente de los datos conocidos\n",
    "    last_date = original_data.index[-1]\n",
    "    \n",
    "    # Crear las fechas correspondientes para las predicciones\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta('1 hour'), periods=len(future_predictions_scaled), freq='H')\n",
    "\n",
    "    # Crear un DataFrame con las fechas y las predicciones escaladas\n",
    "    future_predictions_df = pd.DataFrame(future_predictions_scaled, columns=['Prediction'], index=future_dates)\n",
    "\n",
    "    # Actualizar el DataFrame original con la primera predicción\n",
    "    original_data.loc[future_dates[0]] = future_predictions_scaled[0, 0]\n",
    "    \n",
    "    return future_predictions_df, original_data\n",
    "\n",
    "# Ejemplo de uso\n",
    "nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=True)\n",
    "imputation_aitsu_filled, scaler = fill_missing_values_with_lstm(imputation_aitsu, nan_sequences, lookback=24, forecast_horizon=6, model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 18:16:02,852 - INFO - Iniciando proceso de imputación para la variable: LLUVIA_Aitzu-Urola\n",
      "2024-11-18 18:16:02,853 - INFO - Archivo de log creado en: logs/imputation_LLUVIA_Aitzu-Urola_20241118_181602.log\n",
      "2024-11-18 18:16:02,853 - INFO - Cargando datos...\n",
      "2024-11-18 18:16:03,054 - INFO - Datos cargados: 215143 registros\n",
      "2024-11-18 18:16:03,054 - INFO - Iniciando búsqueda de secuencias de valores NaN\n",
      "2024-11-18 18:16:03,085 - INFO - Secuencia encontrada: long_consecutive - Inicio: 1999-03-16 17:00:00, Fin: 2000-04-05 12:00:00, Valores: 9260\n",
      "2024-11-18 18:16:03,085 - INFO - Secuencia encontrada: single - Inicio: 2000-05-25 08:00:00, Fin: 2000-05-25 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,086 - INFO - Secuencia encontrada: single - Inicio: 2000-12-08 03:00:00, Fin: 2000-12-08 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,086 - INFO - Secuencia encontrada: consecutive - Inicio: 2000-12-12 00:00:00, Fin: 2000-12-12 23:00:00, Valores: 24\n",
      "2024-11-18 18:16:03,086 - INFO - Secuencia encontrada: single - Inicio: 2000-12-15 16:00:00, Fin: 2000-12-15 16:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,087 - INFO - Secuencia encontrada: single - Inicio: 2000-12-23 09:00:00, Fin: 2000-12-23 09:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,087 - INFO - Secuencia encontrada: single - Inicio: 2000-12-23 17:00:00, Fin: 2000-12-23 17:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,087 - INFO - Secuencia encontrada: single - Inicio: 2000-12-23 23:00:00, Fin: 2000-12-23 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,087 - INFO - Secuencia encontrada: single - Inicio: 2000-12-28 15:00:00, Fin: 2000-12-28 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,087 - INFO - Secuencia encontrada: single - Inicio: 2001-02-06 00:00:00, Fin: 2001-02-06 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,088 - INFO - Secuencia encontrada: single - Inicio: 2001-02-07 12:00:00, Fin: 2001-02-07 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,088 - INFO - Secuencia encontrada: single - Inicio: 2001-02-08 04:00:00, Fin: 2001-02-08 04:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,088 - INFO - Secuencia encontrada: single - Inicio: 2001-02-13 01:00:00, Fin: 2001-02-13 01:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,088 - INFO - Secuencia encontrada: single - Inicio: 2001-03-25 22:00:00, Fin: 2001-03-25 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,089 - INFO - Secuencia encontrada: single - Inicio: 2001-06-02 05:00:00, Fin: 2001-06-02 05:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,089 - INFO - Secuencia encontrada: single - Inicio: 2001-09-28 19:00:00, Fin: 2001-09-28 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,089 - INFO - Secuencia encontrada: single - Inicio: 2002-01-08 00:00:00, Fin: 2002-01-08 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,089 - INFO - Secuencia encontrada: single - Inicio: 2002-01-13 11:00:00, Fin: 2002-01-13 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,089 - INFO - Secuencia encontrada: consecutive - Inicio: 2002-02-05 13:00:00, Fin: 2002-02-05 23:00:00, Valores: 11\n",
      "2024-11-18 18:16:03,090 - INFO - Secuencia encontrada: single - Inicio: 2002-03-03 03:00:00, Fin: 2002-03-03 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,090 - INFO - Secuencia encontrada: single - Inicio: 2002-03-18 00:00:00, Fin: 2002-03-18 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,090 - INFO - Secuencia encontrada: single - Inicio: 2002-04-15 07:00:00, Fin: 2002-04-15 07:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,090 - INFO - Secuencia encontrada: single - Inicio: 2002-04-22 05:00:00, Fin: 2002-04-22 05:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,091 - INFO - Secuencia encontrada: single - Inicio: 2002-05-11 03:00:00, Fin: 2002-05-11 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,091 - INFO - Secuencia encontrada: consecutive - Inicio: 2002-05-13 14:00:00, Fin: 2002-05-13 23:00:00, Valores: 10\n",
      "2024-11-18 18:16:03,091 - INFO - Secuencia encontrada: single - Inicio: 2002-06-15 13:00:00, Fin: 2002-06-15 13:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,091 - INFO - Secuencia encontrada: single - Inicio: 2002-07-09 10:00:00, Fin: 2002-07-09 10:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,092 - INFO - Secuencia encontrada: single - Inicio: 2002-11-12 23:00:00, Fin: 2002-11-12 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,092 - INFO - Secuencia encontrada: single - Inicio: 2003-01-17 03:00:00, Fin: 2003-01-17 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,092 - INFO - Secuencia encontrada: consecutive - Inicio: 2003-02-05 12:00:00, Fin: 2003-02-05 23:00:00, Valores: 12\n",
      "2024-11-18 18:16:03,093 - INFO - Secuencia encontrada: single - Inicio: 2003-03-10 12:00:00, Fin: 2003-03-10 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,093 - INFO - Secuencia encontrada: single - Inicio: 2003-04-04 02:00:00, Fin: 2003-04-04 02:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,093 - INFO - Secuencia encontrada: single - Inicio: 2003-04-07 19:00:00, Fin: 2003-04-07 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,093 - INFO - Secuencia encontrada: single - Inicio: 2003-04-10 17:00:00, Fin: 2003-04-10 17:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,093 - INFO - Secuencia encontrada: single - Inicio: 2003-04-13 13:00:00, Fin: 2003-04-13 13:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,094 - INFO - Secuencia encontrada: single - Inicio: 2003-04-15 16:00:00, Fin: 2003-04-15 16:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,094 - INFO - Secuencia encontrada: single - Inicio: 2003-04-23 15:00:00, Fin: 2003-04-23 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,094 - INFO - Secuencia encontrada: single - Inicio: 2003-04-27 08:00:00, Fin: 2003-04-27 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,094 - INFO - Secuencia encontrada: single - Inicio: 2003-10-18 08:00:00, Fin: 2003-10-18 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,094 - INFO - Secuencia encontrada: single - Inicio: 2003-10-18 12:00:00, Fin: 2003-10-18 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,095 - INFO - Secuencia encontrada: single - Inicio: 2003-10-19 23:00:00, Fin: 2003-10-19 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,095 - INFO - Secuencia encontrada: single - Inicio: 2003-10-30 11:00:00, Fin: 2003-10-30 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,095 - INFO - Secuencia encontrada: single - Inicio: 2003-12-08 00:00:00, Fin: 2003-12-08 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,095 - INFO - Secuencia encontrada: single - Inicio: 2003-12-08 06:00:00, Fin: 2003-12-08 06:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,095 - INFO - Secuencia encontrada: single - Inicio: 2003-12-08 08:00:00, Fin: 2003-12-08 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,096 - INFO - Secuencia encontrada: single - Inicio: 2003-12-08 10:00:00, Fin: 2003-12-08 10:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,096 - INFO - Secuencia encontrada: single - Inicio: 2003-12-08 23:00:00, Fin: 2003-12-08 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,096 - INFO - Secuencia encontrada: single - Inicio: 2003-12-23 06:00:00, Fin: 2003-12-23 06:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,096 - INFO - Secuencia encontrada: single - Inicio: 2003-12-30 00:00:00, Fin: 2003-12-30 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,097 - INFO - Secuencia encontrada: consecutive - Inicio: 2003-12-30 04:00:00, Fin: 2003-12-30 06:00:00, Valores: 3\n",
      "2024-11-18 18:16:03,097 - INFO - Secuencia encontrada: consecutive - Inicio: 2003-12-30 09:00:00, Fin: 2003-12-30 10:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,097 - INFO - Secuencia encontrada: consecutive - Inicio: 2003-12-30 16:00:00, Fin: 2003-12-30 18:00:00, Valores: 3\n",
      "2024-11-18 18:16:03,097 - INFO - Secuencia encontrada: single - Inicio: 2003-12-30 21:00:00, Fin: 2003-12-30 21:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,098 - INFO - Secuencia encontrada: single - Inicio: 2003-12-30 23:00:00, Fin: 2003-12-30 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,098 - INFO - Secuencia encontrada: single - Inicio: 2004-01-17 03:00:00, Fin: 2004-01-17 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,098 - INFO - Secuencia encontrada: single - Inicio: 2004-02-04 10:00:00, Fin: 2004-02-04 10:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,098 - INFO - Secuencia encontrada: single - Inicio: 2004-03-24 01:00:00, Fin: 2004-03-24 01:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,098 - INFO - Secuencia encontrada: single - Inicio: 2004-04-19 11:00:00, Fin: 2004-04-19 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,099 - INFO - Secuencia encontrada: single - Inicio: 2004-05-08 17:00:00, Fin: 2004-05-08 17:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,099 - INFO - Secuencia encontrada: single - Inicio: 2004-05-19 08:00:00, Fin: 2004-05-19 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,099 - INFO - Secuencia encontrada: single - Inicio: 2004-07-07 15:00:00, Fin: 2004-07-07 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,099 - INFO - Secuencia encontrada: single - Inicio: 2004-09-21 14:00:00, Fin: 2004-09-21 14:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,099 - INFO - Secuencia encontrada: single - Inicio: 2004-11-04 08:00:00, Fin: 2004-11-04 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,100 - INFO - Secuencia encontrada: single - Inicio: 2005-05-22 11:00:00, Fin: 2005-05-22 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,100 - INFO - Secuencia encontrada: single - Inicio: 2005-05-26 22:00:00, Fin: 2005-05-26 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,100 - INFO - Secuencia encontrada: single - Inicio: 2005-08-04 19:00:00, Fin: 2005-08-04 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,101 - INFO - Secuencia encontrada: single - Inicio: 2006-02-07 22:00:00, Fin: 2006-02-07 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,101 - INFO - Secuencia encontrada: single - Inicio: 2006-02-08 07:00:00, Fin: 2006-02-08 07:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,101 - INFO - Secuencia encontrada: single - Inicio: 2006-02-10 16:00:00, Fin: 2006-02-10 16:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,101 - INFO - Secuencia encontrada: single - Inicio: 2006-03-22 12:00:00, Fin: 2006-03-22 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,101 - INFO - Secuencia encontrada: single - Inicio: 2006-04-04 23:00:00, Fin: 2006-04-04 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2006-08-28 07:00:00, Fin: 2006-08-28 07:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2006-09-08 02:00:00, Fin: 2006-09-08 02:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2006-09-13 20:00:00, Fin: 2006-09-13 20:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2006-09-22 09:00:00, Fin: 2006-09-22 09:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2006-12-10 00:00:00, Fin: 2006-12-10 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,102 - INFO - Secuencia encontrada: single - Inicio: 2007-02-22 22:00:00, Fin: 2007-02-22 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,103 - INFO - Secuencia encontrada: single - Inicio: 2007-03-04 15:00:00, Fin: 2007-03-04 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,103 - INFO - Secuencia encontrada: single - Inicio: 2007-04-01 05:00:00, Fin: 2007-04-01 05:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,103 - INFO - Secuencia encontrada: single - Inicio: 2007-06-26 15:00:00, Fin: 2007-06-26 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,103 - INFO - Secuencia encontrada: single - Inicio: 2007-08-08 00:00:00, Fin: 2007-08-08 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,103 - INFO - Secuencia encontrada: single - Inicio: 2007-08-08 18:00:00, Fin: 2007-08-08 18:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,104 - INFO - Secuencia encontrada: single - Inicio: 2007-08-10 03:00:00, Fin: 2007-08-10 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,104 - INFO - Secuencia encontrada: single - Inicio: 2007-08-10 08:00:00, Fin: 2007-08-10 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,104 - INFO - Secuencia encontrada: single - Inicio: 2007-08-13 22:00:00, Fin: 2007-08-13 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,104 - INFO - Secuencia encontrada: single - Inicio: 2007-08-14 10:00:00, Fin: 2007-08-14 10:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,104 - INFO - Secuencia encontrada: single - Inicio: 2007-08-17 02:00:00, Fin: 2007-08-17 02:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,105 - INFO - Secuencia encontrada: single - Inicio: 2007-08-17 22:00:00, Fin: 2007-08-17 22:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,105 - INFO - Secuencia encontrada: single - Inicio: 2007-08-19 15:00:00, Fin: 2007-08-19 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,105 - INFO - Secuencia encontrada: single - Inicio: 2007-08-21 06:00:00, Fin: 2007-08-21 06:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,105 - INFO - Secuencia encontrada: single - Inicio: 2007-08-21 09:00:00, Fin: 2007-08-21 09:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,105 - INFO - Secuencia encontrada: single - Inicio: 2007-08-23 08:00:00, Fin: 2007-08-23 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,106 - INFO - Secuencia encontrada: single - Inicio: 2007-09-04 15:00:00, Fin: 2007-09-04 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,106 - INFO - Secuencia encontrada: single - Inicio: 2007-09-04 19:00:00, Fin: 2007-09-04 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,107 - INFO - Secuencia encontrada: single - Inicio: 2007-09-10 04:00:00, Fin: 2007-09-10 04:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,107 - INFO - Secuencia encontrada: single - Inicio: 2007-12-09 00:00:00, Fin: 2007-12-09 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,108 - INFO - Secuencia encontrada: single - Inicio: 2008-01-10 19:00:00, Fin: 2008-01-10 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,108 - INFO - Secuencia encontrada: single - Inicio: 2008-03-23 00:00:00, Fin: 2008-03-23 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,109 - INFO - Secuencia encontrada: single - Inicio: 2008-03-23 18:00:00, Fin: 2008-03-23 18:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,109 - INFO - Secuencia encontrada: consecutive - Inicio: 2008-03-23 22:00:00, Fin: 2008-03-23 23:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,109 - INFO - Secuencia encontrada: single - Inicio: 2008-06-02 11:00:00, Fin: 2008-06-02 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,110 - INFO - Secuencia encontrada: single - Inicio: 2008-06-06 08:00:00, Fin: 2008-06-06 08:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,110 - INFO - Secuencia encontrada: consecutive - Inicio: 2008-06-21 22:00:00, Fin: 2008-06-25 08:00:00, Valores: 83\n",
      "2024-11-18 18:16:03,110 - INFO - Secuencia encontrada: single - Inicio: 2008-07-16 20:00:00, Fin: 2008-07-16 20:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,110 - INFO - Secuencia encontrada: single - Inicio: 2008-07-22 03:00:00, Fin: 2008-07-22 03:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,111 - INFO - Secuencia encontrada: single - Inicio: 2008-07-23 10:00:00, Fin: 2008-07-23 10:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,111 - INFO - Secuencia encontrada: single - Inicio: 2008-07-30 04:00:00, Fin: 2008-07-30 04:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,111 - INFO - Secuencia encontrada: single - Inicio: 2008-07-30 21:00:00, Fin: 2008-07-30 21:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,111 - INFO - Secuencia encontrada: single - Inicio: 2008-07-31 02:00:00, Fin: 2008-07-31 02:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,112 - INFO - Secuencia encontrada: single - Inicio: 2008-08-02 12:00:00, Fin: 2008-08-02 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,112 - INFO - Secuencia encontrada: consecutive - Inicio: 2008-11-12 13:00:00, Fin: 2008-11-12 14:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,112 - INFO - Secuencia encontrada: single - Inicio: 2009-02-05 00:00:00, Fin: 2009-02-05 00:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,112 - INFO - Secuencia encontrada: single - Inicio: 2009-02-07 02:00:00, Fin: 2009-02-07 02:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,113 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-04-29 11:00:00, Fin: 2009-04-29 23:00:00, Valores: 13\n",
      "2024-11-18 18:16:03,113 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-05-29 10:00:00, Fin: 2009-05-29 23:00:00, Valores: 14\n",
      "2024-11-18 18:16:03,113 - INFO - Secuencia encontrada: single - Inicio: 2009-06-02 13:00:00, Fin: 2009-06-02 13:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,113 - INFO - Secuencia encontrada: single - Inicio: 2009-06-09 20:00:00, Fin: 2009-06-09 20:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,114 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-06-29 22:00:00, Fin: 2009-06-30 12:00:00, Valores: 15\n",
      "2024-11-18 18:16:03,114 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-06-30 14:00:00, Fin: 2009-06-30 16:00:00, Valores: 3\n",
      "2024-11-18 18:16:03,114 - INFO - Secuencia encontrada: single - Inicio: 2009-07-12 09:00:00, Fin: 2009-07-12 09:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,114 - INFO - Secuencia encontrada: single - Inicio: 2009-07-29 20:00:00, Fin: 2009-07-29 20:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,114 - INFO - Secuencia encontrada: single - Inicio: 2009-10-22 23:00:00, Fin: 2009-10-22 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-12-09 09:00:00, Fin: 2009-12-10 12:00:00, Valores: 28\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: consecutive - Inicio: 2009-12-11 11:00:00, Fin: 2009-12-11 12:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: single - Inicio: 2009-12-16 09:00:00, Fin: 2009-12-16 09:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: single - Inicio: 2010-02-09 12:00:00, Fin: 2010-02-09 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: single - Inicio: 2010-02-17 11:00:00, Fin: 2010-02-17 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,115 - INFO - Secuencia encontrada: consecutive - Inicio: 2010-02-18 15:00:00, Fin: 2010-02-18 16:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,116 - INFO - Secuencia encontrada: consecutive - Inicio: 2011-03-21 16:00:00, Fin: 2011-03-22 10:00:00, Valores: 19\n",
      "2024-11-18 18:16:03,116 - INFO - Secuencia encontrada: consecutive - Inicio: 2011-10-05 11:00:00, Fin: 2011-10-05 23:00:00, Valores: 13\n",
      "2024-11-18 18:16:03,116 - INFO - Secuencia encontrada: consecutive - Inicio: 2013-07-07 23:00:00, Fin: 2013-07-08 04:00:00, Valores: 6\n",
      "2024-11-18 18:16:03,116 - INFO - Secuencia encontrada: consecutive - Inicio: 2016-08-19 11:00:00, Fin: 2016-08-19 13:00:00, Valores: 3\n",
      "2024-11-18 18:16:03,117 - INFO - Secuencia encontrada: consecutive - Inicio: 2019-04-25 15:00:00, Fin: 2019-04-26 06:00:00, Valores: 16\n",
      "2024-11-18 18:16:03,117 - INFO - Secuencia encontrada: single - Inicio: 2020-07-02 12:00:00, Fin: 2020-07-02 12:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,118 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-06-28 09:00:00, Fin: 2021-06-30 07:00:00, Valores: 47\n",
      "2024-11-18 18:16:03,118 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-07-22 08:00:00, Fin: 2021-07-23 08:00:00, Valores: 25\n",
      "2024-11-18 18:16:03,119 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-09-06 02:00:00, Fin: 2021-09-06 09:00:00, Valores: 8\n",
      "2024-11-18 18:16:03,119 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-09-06 11:00:00, Fin: 2021-09-07 07:00:00, Valores: 21\n",
      "2024-11-18 18:16:03,119 - INFO - Secuencia encontrada: single - Inicio: 2021-10-23 06:00:00, Fin: 2021-10-23 06:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,119 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-10-23 10:00:00, Fin: 2021-10-23 11:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-19 07:00:00, Fin: 2021-12-19 10:00:00, Valores: 4\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-19 12:00:00, Fin: 2021-12-19 13:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: single - Inicio: 2021-12-19 15:00:00, Fin: 2021-12-19 15:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: single - Inicio: 2021-12-19 23:00:00, Fin: 2021-12-19 23:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-20 03:00:00, Fin: 2021-12-20 05:00:00, Valores: 3\n",
      "2024-11-18 18:16:03,120 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-20 09:00:00, Fin: 2021-12-20 16:00:00, Valores: 8\n",
      "2024-11-18 18:16:03,121 - INFO - Secuencia encontrada: single - Inicio: 2021-12-20 19:00:00, Fin: 2021-12-20 19:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,121 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-21 00:00:00, Fin: 2021-12-21 01:00:00, Valores: 2\n",
      "2024-11-18 18:16:03,121 - INFO - Secuencia encontrada: single - Inicio: 2021-12-21 04:00:00, Fin: 2021-12-21 04:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,121 - INFO - Secuencia encontrada: consecutive - Inicio: 2021-12-21 06:00:00, Fin: 2021-12-22 08:00:00, Valores: 27\n",
      "2024-11-18 18:16:03,122 - INFO - Secuencia encontrada: long_consecutive - Inicio: 2022-01-04 00:00:00, Fin: 2022-01-10 10:00:00, Valores: 155\n",
      "2024-11-18 18:16:03,122 - INFO - Secuencia encontrada: single - Inicio: 2022-01-11 11:00:00, Fin: 2022-01-11 11:00:00, Valores: 1\n",
      "2024-11-18 18:16:03,122 - INFO - Secuencia encontrada: consecutive - Inicio: 2022-05-22 22:00:00, Fin: 2022-05-23 06:00:00, Valores: 9\n",
      "2024-11-18 18:16:03,123 - INFO - Total de secuencias encontradas: 154\n",
      "2024-11-18 18:16:03,123 - WARNING - Secuencia muy larga detectada: 1999-03-16 17:00:00 a 2000-04-05 12:00:00, 9260 valores\n",
      "2024-11-18 18:16:03,123 - INFO - Procesando secuencia: 2000-05-25 08:00:00 a 2000-05-25 08:00:00 (1 valores)\n",
      "2024-11-18 18:16:03,125 - INFO - Entrenando modelo para predecir valores faltantes desde 2000-05-25 08:00:00 hasta 2000-05-25 08:00:00: (1 valores faltantes)\n",
      "2024-11-18 18:16:03,125 - INFO - ---------- Iniciando entrenamiento del modelo ----------\n",
      "2024-11-18 18:16:03,125 - INFO - Parámetros - Lookback: 24, Forecast Horizon: 6\n",
      "2024-11-18 18:16:03,125 - INFO - División de datos - Train: 836, Validation: 179, Test: 180\n",
      "2024-11-18 18:16:03,136 - INFO - Secuencias creadas - Train: 807, Val: 150, Test: 151\n",
      "2024-11-18 18:16:03,136 - INFO - Longitud de x_train: 807\n",
      "2024-11-18 18:16:03,136 - INFO - Longitud de x_val:' 150\n",
      "2024-11-18 18:16:03,136 - INFO - Longitud de x_test:' 151\n",
      "2024-11-18 18:16:03,137 - INFO - Dispositivo utilizado: cpu\n",
      "/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuecia nan numero: 0\n",
      "Secuecia nan numero: 0\n",
      "start_train_date:  2000-04-05 13:00:00\n",
      "end_date:  2000-05-25 08:00:00\n",
      "2000-04-05 13:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/9gnxy1111nj02f15tyvssxk80000gn/T/ipykernel_12991/1775212097.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_temp.pth'))\n",
      "2024-11-18 18:16:06,235 - INFO - ----- Prediciendo valores faltantes desde 2000-05-25 08:00:00 hasta 2000-05-25 08:00:00\n",
      "/var/folders/dm/9gnxy1111nj02f15tyvssxk80000gn/T/ipykernel_12991/1775212097.py:270: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  missing_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
      "2024-11-18 18:16:06,236 - INFO - Predicción completada: 1 valores generados\n",
      "2024-11-18 18:16:06,236 - INFO - Actualizando DataFrame con valores predichos\n",
      "2024-11-18 18:16:06,238 - INFO - **************************************************\n",
      "2024-11-18 18:16:06,239 - INFO - Procesando secuencia: 2000-12-08 03:00:00 a 2000-12-08 03:00:00 (1 valores)\n",
      "2024-11-18 18:16:06,239 - INFO - Entrenando modelo para predecir valores faltantes desde 2000-12-08 03:00:00 hasta 2000-12-08 03:00:00: (1 valores faltantes)\n",
      "2024-11-18 18:16:06,239 - INFO - ---------- Iniciando entrenamiento del modelo ----------\n",
      "2024-11-18 18:16:06,240 - INFO - Parámetros - Lookback: 24, Forecast Horizon: 6\n",
      "2024-11-18 18:16:06,240 - INFO - División de datos - Train: 4142, Validation: 887, Test: 889\n",
      "2024-11-18 18:16:06,296 - INFO - Secuencias creadas - Train: 4113, Val: 858, Test: 860\n",
      "2024-11-18 18:16:06,297 - INFO - Longitud de x_train: 4113\n",
      "2024-11-18 18:16:06,297 - INFO - Longitud de x_val:' 858\n",
      "2024-11-18 18:16:06,297 - INFO - Longitud de x_test:' 860\n",
      "2024-11-18 18:16:06,297 - INFO - Dispositivo utilizado: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 34\n",
      "\n",
      "Entrenamiento completado en 3.10 segundos\n",
      "Tiempo promedio por época: 0.00 segundos\n",
      "Best model was saved at epoch 14 with validation RMSE: 0.0001\n",
      "Secuecia nan numero: 1\n",
      "start_train_date:  2000-04-05 13:00:00\n",
      "end_date:  2000-12-08 03:00:00\n",
      "2000-04-05 13:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 367\u001b[0m\n\u001b[1;32m    364\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando modelo para predecir valores faltantes desde \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hasta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(num_nans)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m valores faltantes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m model, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_river\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Predecir los valores faltantes\u001b[39;00m\n\u001b[1;32m    370\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m predict_missing_values(model, scaler, imputation_aitsu, start_date, end_date, lookback\u001b[38;5;241m=\u001b[39mlookback, forecast_horizon\u001b[38;5;241m=\u001b[39mforecast_horizon)\n",
      "Cell \u001b[0;32mIn[2], line 206\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_river, lookback, forecast_horizon)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    205\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 206\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m    208\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m, in \u001b[0;36mImprovedLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    112\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# %%\n",
    "# Definimos las funciones necesarias para el preprocesamiento y análisis de los datos\n",
    "def create_sequences(data, lookback=24, forecast_horizon=6):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        x = data[i:(i + lookback)]\n",
    "        y = data[(i + lookback):(i + lookback + forecast_horizon)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return torch.tensor(xs, dtype=torch.float32), torch.tensor(ys, dtype=torch.float32)\n",
    "\n",
    "def find_nan_sequences(df, max_consecutive_nans=100, view=False):\n",
    "    \"\"\"\n",
    "    Encuentra secuencias de valores faltantes en el DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(\"Iniciando búsqueda de secuencias de valores NaN\")\n",
    "    nan_index = df[df.isnull().any(axis=1)].index\n",
    "    nan_sequences = []\n",
    "    \n",
    "    if len(nan_index) == 0:\n",
    "        logging.info(\"No se encontraron valores NaN en el DataFrame\")\n",
    "        return nan_sequences\n",
    "\n",
    "    start_date = nan_index[0]\n",
    "    prev_date = nan_index[0]\n",
    "    \n",
    "    for i in range(1, len(nan_index)):\n",
    "        current_date = nan_index[i]\n",
    "        \n",
    "        if current_date - prev_date != pd.Timedelta('1 hour 00:00:00'):\n",
    "            end_date = prev_date\n",
    "            num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "            \n",
    "            if num_nans == 1:\n",
    "                sequence_type = 'single'\n",
    "            elif num_nans <= max_consecutive_nans:\n",
    "                sequence_type = 'consecutive'\n",
    "            else:\n",
    "                sequence_type = 'long_consecutive'\n",
    "            \n",
    "            nan_sequences.append({\n",
    "                'type': sequence_type,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'num_nans': num_nans\n",
    "            })\n",
    "            \n",
    "            logging.info(f\"Secuencia encontrada: {sequence_type} - Inicio: {start_date}, Fin: {end_date}, Valores: {int(num_nans)}\")\n",
    "            start_date = current_date\n",
    "        \n",
    "        prev_date = current_date\n",
    "    \n",
    "    # Procesar la última secuencia\n",
    "    end_date = prev_date\n",
    "    num_nans = (end_date - start_date).total_seconds() / 3600 + 1\n",
    "    sequence_type = 'single' if num_nans == 1 else 'consecutive' if num_nans <= max_consecutive_nans else 'long_consecutive'\n",
    "    \n",
    "    nan_sequences.append({\n",
    "        'type': sequence_type,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'num_nans': num_nans\n",
    "    })\n",
    "    \n",
    "    logging.info(f\"Total de secuencias encontradas: {len(nan_sequences)}\")\n",
    "    return nan_sequences\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # mse = ((y_true - y_pred) ** 2).mean()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mae, mape\n",
    "\n",
    "# %%\n",
    "# Definir el modelo LSTM mejorado\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)  \n",
    "        self.fc = nn.Linear(hidden_size, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(32, output_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# %%\n",
    "# Función para entrenar el modelo con los datos disponibles\n",
    "def train_model(data_river, lookback=24, forecast_horizon=6):\n",
    "    logging.info(\"---------- Iniciando entrenamiento del modelo ----------\")\n",
    "    logging.info(f\"Parámetros - Lookback: {lookback}, Forecast Horizon: {forecast_horizon}\")\n",
    "    total_size = len(data_river)\n",
    "    train_size = int(total_size * 0.70)\n",
    "    val_size = int(total_size * 0.15)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    train = data_river[:train_size]\n",
    "    val = data_river[train_size:train_size + val_size]\n",
    "    test = data_river[train_size + val_size:]\n",
    "    logging.info(f\"División de datos - Train: {len(train)}, Validation: {len(val)}, Test: {len(test)}\")\n",
    "    \n",
    "    # Normalización\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(train.values)\n",
    "\n",
    "    # Transformar los conjuntos\n",
    "    train_scaled = scaler.transform(train.values)\n",
    "    val_scaled = scaler.transform(val.values)\n",
    "    test_scaled = scaler.transform(test.values)\n",
    "\n",
    "    # Crear secuencias\n",
    "    x_train, y_train = create_sequences(train_scaled, lookback, forecast_horizon)\n",
    "    x_val, y_val = create_sequences(val_scaled, lookback, forecast_horizon)\n",
    "    x_test, y_test = create_sequences(test_scaled, lookback, forecast_horizon)\n",
    "    \n",
    "    logging.info(f\"Secuencias creadas - Train: {len(x_train)}, Val: {len(x_val)}, Test: {len(x_test)}\")\n",
    "\n",
    "    \n",
    "    logging.info(f\"Longitud de x_train: {len(x_train)}\")\n",
    "    logging.info(f\"Longitud de x_val:' {len(x_val)}\")\n",
    "    logging.info(f\"Longitud de x_test:' {len(x_test)}\")\n",
    "\n",
    "    # Configuración del modelo y entrenamiento\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "    # Convertir a tensores\n",
    "    x_train = x_train.to(device).float()    \n",
    "    y_train = y_train.to(device).float()\n",
    "    x_val = x_val.to(device).float()\n",
    "    y_val = y_val.to(device).float()\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    input_size = 1\n",
    "    hidden_size = 128\n",
    "    num_layers = 1\n",
    "    dropout = 0.1\n",
    "\n",
    "    model = ImprovedLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_horizon=forecast_horizon, dropout=dropout)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Parámetros de entrenamiento\n",
    "    weight_decay = 1e-5\n",
    "    lr = 1e-2\n",
    "    batch_size = 64\n",
    "    scheduler_patience = 15\n",
    "    early_stopping_patience = 20\n",
    "    n_epochs = 1000\n",
    "    validation_freq = 1\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=scheduler_patience, threshold=0.00001)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    val_rmse = []\n",
    "    val_maes = []\n",
    "    val_mapes = []\n",
    "    learning_rates = []\n",
    "    counter = 0\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range (n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        epoch_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validación\n",
    "        if epoch % validation_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_val = model(x_val)\n",
    "                val_loss = loss_fn(y_pred_val, y_val.squeeze())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                y_true = scaler.inverse_transform(y_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "                y_pred_np = scaler.inverse_transform(y_pred_val.cpu().numpy().reshape(-1, forecast_horizon))\n",
    "                rmse, mae, mape = calculate_metrics(y_true.flatten(), y_pred_np.flatten())\n",
    "                val_rmse.append(rmse)\n",
    "                val_maes.append(mae)\n",
    "                val_mapes.append(mape)\n",
    "\n",
    "        scheduler.step(val_loss.item())\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            counter = 0\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            torch.save(best_model, 'best_model_temp.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\nEntrenamiento completado en {total_time:.2f} segundos')\n",
    "    print(f'Tiempo promedio por época: {total_time/n_epochs:.2f} segundos')\n",
    "    print(f\"Best model was saved at epoch {best_epoch} with validation RMSE: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Cargar el mejor modelo\n",
    "    model.load_state_dict(torch.load('best_model_temp.pth'))\n",
    "\n",
    "    # save_images_train_model(epoch, train_losses, val_losses, val_rmse, val_maes, val_mapes, learning_rates)\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "# %%\n",
    "# Función para predecir los valores faltantes\n",
    "def predict_missing_values(model, scaler, imputation_aitsu, start_date, end_date, lookback=24, forecast_horizon=6):\n",
    "    logging.info(f\"----- Prediciendo valores faltantes desde {start_date} hasta {end_date}\")\n",
    "    data_before = imputation_aitsu[:start_date - pd.Timedelta('1 hour')]\n",
    "    input_data = data_before[-lookback:].values\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    missing_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    predicted_values = []\n",
    "    \n",
    "    x_input = torch.tensor(input_scaled.reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while len(predicted_values) < len(missing_dates):\n",
    "            y_pred = model(x_input).cpu().numpy()\n",
    "            y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "            predicted_values.extend(y_pred_inv.flatten())\n",
    "            \n",
    "            last_values = np.concatenate((x_input.cpu().numpy().flatten()[forecast_horizon:], y_pred.flatten()))\n",
    "            x_input = torch.tensor(last_values[-lookback:].reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "    \n",
    "    predicted_values = predicted_values[:len(missing_dates)]\n",
    "    df_pred = pd.DataFrame(data=predicted_values, index=missing_dates, columns=[variable])\n",
    "    \n",
    "    logging.info(f\"Predicción completada: {len(predicted_values)} valores generados\")\n",
    "    return df_pred\n",
    "\n",
    "def setup_logging(variable_name):\n",
    "    \"\"\"\n",
    "    Configura el sistema de logging para el proceso de imputación.\n",
    "    \"\"\"\n",
    "    # Crear directorio para logs si no existe\n",
    "    log_dir = Path('logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Crear nombre del archivo de log\n",
    "    log_filename = log_dir / f'imputation_{variable_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "    \n",
    "    # Configurar el logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()  # Para mostrar también en consola\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Iniciando proceso de imputación para la variable: {variable_name}\")\n",
    "    logging.info(f\"Archivo de log creado en: {log_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Código principal\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    variable = 'LLUVIA_Aitzu-Urola'\n",
    "    setup_logging(variable)\n",
    "    logging.info(\"Cargando datos...\")\n",
    "    data_river_2 = pd.read_csv('db_21.csv')\n",
    "    data_river_2 = data_river_2.set_index('Fecha')\n",
    "    data_river_2.index = pd.to_datetime(data_river_2.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    imputation_aitsu = data_river_2[[variable]].copy()\n",
    "    date_init = '1999-03-16 17:00'\n",
    "    date_end = '2023-09-30 23:00'\n",
    "    imputation_aitsu = imputation_aitsu[date_init:date_end] \n",
    "    logging.info(f\"Datos cargados: {len(imputation_aitsu)} registros\")\n",
    "    # %%\n",
    "    # Encontrar las secuencias de valores faltantes\n",
    "    nan_sequences = find_nan_sequences(imputation_aitsu, max_consecutive_nans=120, view=False)\n",
    "\n",
    "    # %%\n",
    "    # Proceso iterativo para completar los valores faltantes\n",
    "    lookback = 24\n",
    "    forecast_horizon = 6\n",
    "    aux = 0\n",
    "    start_train_date = imputation_aitsu.index[0]\n",
    "    for seq in nan_sequences:\n",
    "        print(f\"Secuecia nan numero: {aux}\")\n",
    "        if seq['type'] == 'long_consecutive':\n",
    "            logging.warning(f\"Secuencia muy larga detectada: {seq['start_date']} a {seq['end_date']}, {int(seq['num_nans'])} valores\")\n",
    "            start_train_date = seq['end_date'] + pd.Timedelta('1 hour')\n",
    "        else:\n",
    "            start_date = seq['start_date']\n",
    "            end_date = seq['end_date']\n",
    "            num_nans = seq['num_nans']\n",
    "            logging.info(f\"Procesando secuencia: {start_date} a {end_date} ({int(num_nans)} valores)\")\n",
    "\n",
    "            # Obtener los datos hasta la secuencia faltante\n",
    "            data_river = imputation_aitsu[start_train_date:start_date - pd.Timedelta('1 hour')]\n",
    "            print(\"start_train_date: \", start_train_date)\n",
    "            print(\"end_date: \", start_date)\n",
    "            print(data_river.index[0])\n",
    "\n",
    "            # Verificar si hay suficientes datos para entrenar\n",
    "            if len(data_river) < lookback + forecast_horizon:\n",
    "                logging.warning(f\"Datos insuficientes para entrenar antes de {start_date}\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Entrenando modelo para predecir valores faltantes desde {start_date} hasta {end_date}: ({int(num_nans)} valores faltantes)\")\n",
    "\n",
    "            # Entrenar el modelo\n",
    "            model, scaler = train_model(data_river, lookback, forecast_horizon)\n",
    "\n",
    "            # Predecir los valores faltantes\n",
    "            df_pred = predict_missing_values(model, scaler, imputation_aitsu, start_date, end_date, lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "\n",
    "            # Actualizar el DataFrame principal con los valores predichos\n",
    "            logging.info(f\"Actualizando DataFrame con valores predichos\")\n",
    "            imputation_aitsu.update(df_pred)\n",
    "            aux += 1\n",
    "            logging.info(f\"*\"*100)\n",
    "\n",
    "# %%\n",
    "    # Verificar si quedan valores faltantes\n",
    "    remaining_nans = imputation_aitsu.isna().sum()\n",
    "    logging.info(f\"Proceso completado. Valores faltantes restantes: {remaining_nans}\")\n",
    "\n",
    "# %%\n",
    "    # Guardar el DataFrame con los valores completados\n",
    "    output_file = f'imputation_caudal{variable}_completed.csv'\n",
    "    imputation_aitsu.to_csv(output_file)\n",
    "    logging.info(f\"Resultados guardados en: {output_file}\")\n",
    "        \n",
    "# Opcional: graficar los datos originales y los datos completados\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(imputation_aitsu.index, imputation_aitsu[variable], label='Datos con imputación', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Datos con valores imputados')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel del Río')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urola_Data = pd.read_csv(\"imputation_CAUDAL_RIO_Ibaieder_completed.csv\")\n",
    "urola_Data = urola_Data.set_index('Fecha')\n",
    "urola_Data.index = pd.to_datetime(urola_Data.index, format='%Y-%m-%d %H:%M:%S')\n",
    "urola_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"2021-10-13\"\n",
    "end = \"2021-11-02\"\n",
    "plt.plot(urola_Data[init:end][variable])\n",
    "plt.plot(data_river_2[init:end][variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(urola_Data.isna().sum())\n",
    "nan_sequences1 = find_nan_sequences(urola_Data, max_consecutive_nans=120, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_date_range(model_path, scaler, imputation_aitsu, start_date, end_date, lookback=100, forecast_horizon=6):\n",
    "    class ImprovedLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_horizon, dropout):\n",
    "            super(ImprovedLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_size, 32)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.output = nn.Linear(32, output_horizon)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.output(out)\n",
    "            return out\n",
    "\n",
    "    # Convert start_date and end_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Load the saved model\n",
    "    model = ImprovedLSTM(input_size=1, hidden_size=256, num_layers=3, output_horizon=forecast_horizon, dropout=0.2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare data up to the prediction start date\n",
    "    data_before = imputation_aitsu[:start_date - pd.Timedelta('1 hour')]\n",
    "    input_data = data_before[-lookback:].values\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "\n",
    "    # Set up the input tensor for prediction\n",
    "    missing_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    predicted_values = []\n",
    "    x_input = torch.tensor(input_scaled.reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        while len(predicted_values) < len(missing_dates):\n",
    "            y_pred = model(x_input).cpu().numpy()\n",
    "            y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "            predicted_values.extend(y_pred_inv.flatten())\n",
    "\n",
    "            # Update input for the next prediction\n",
    "            last_values = np.concatenate((x_input.cpu().numpy().flatten()[forecast_horizon:], y_pred.flatten()))\n",
    "            x_input = torch.tensor(last_values[-lookback:].reshape(1, lookback, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "    predicted_values = predicted_values[:len(missing_dates)]\n",
    "    df_pred = pd.DataFrame(data=predicted_values, index=missing_dates, columns=[variable])\n",
    "\n",
    "    return df_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2021-10-13'\n",
    "end_date = '2021-11-02'\n",
    "df_pred = predict_date_range('best_model_temp.pth', scaler, urola_Data, start_date, end_date)\n",
    "print(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(urola_Data[start_date:end_date][variable], label='Datos reales', alpha=0.7)\n",
    "plt.plot(df_pred[variable], label='Predicciones', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Predicción de valores faltantes')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Nivel del Río')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
